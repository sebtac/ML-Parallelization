{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81a8f9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "############ ANALYSIS OF PARALLELIZATION OF XGBOOST-ON-RAY #############\n",
    "\n",
    "FINDINGS:\n",
    "    \n",
    "- BEST SETUP:\n",
    "    - USE GPUs\n",
    "        - as many actors as GPUs\n",
    "        - USE MAX(AVAILABLE! cpus_per_actor)\n",
    "    - USE ray.init() at the beging of the code\n",
    "    - USE as many file.shardes as GPUs or multiple of GPUs (possibly some minor improvment achived when using a lot of smaller files)\n",
    "    - USE RayDMatrix(..., distributed=True)\n",
    "\n",
    "- GENERAL:\n",
    "    - USE ray.init() at the beging of the code:\n",
    "        - for some reason it does speed up execution, \n",
    "        - although it is discussed nowhere in the documentation or other on-line help materials\n",
    "    - DATA SOURCE:\n",
    "        - RECOMMENDED - (MULTIPLE) PARQUET files\n",
    "            - Thus installation of pyarrow\n",
    "            - number of shards defines available prallelization level            \n",
    "        - INPUT SIZE:\n",
    "            - the bigger the clearer impact of prallelization\n",
    "    - USE RayDMatrix(..., distributed=True)\n",
    "    - USE as many file.shardes as GPUs or multiple of GPUs (possibly some minor improvment achived when using a lot of smaller files)             \n",
    "    - n_jobs & num_actors has no impact on execution time -- often it makes it even worse probabaly due to comm-overhead\n",
    "        \n",
    "- CPU SPECIFIC:\n",
    "    - \"no need to specify the cpu_per_actor as RAY utilizes multithreading and thus uses all avalube CPUs as default\":\n",
    "        - Lost reference but made by a member of the RAY.CORE team\n",
    "        - indeed ON CPU when working with non-distributed dataset specifying this parameter has no significant impact on execution time\n",
    "    - num_actors * cpus_per_actor <= num_CPU_cores:\n",
    "\n",
    "- GPU SPECIFIC:\n",
    "    - EVEN SINGLE GPU HAS FASTER EXECUTION THAN MULT_CPU INSTANCE\n",
    "    - 4 GPUs has 3x SPEEDUP over 1 GPU\n",
    "    \n",
    "- ESTIMATORS:\n",
    "    - RayXGBClassifier(n_jobs=???) - n_jobs param has no impact on execution time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15089d-fe75-4be6-9631-6e656bb23444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENV PREPARATION & ANALYSIS\n",
    "\n",
    "!pip install \"xgboost_ray\"\n",
    "!pip install \"pyarrow\"\n",
    "\n",
    "#display(ray.cluster_resources())\n",
    "#print(ray.cluster_resources()['CPU'], \n",
    "#ray.cluster_resources()['GPU'])\n",
    "#display(dir(ray_params.get_tune_resources()))\n",
    "#ray_params.get_tune_resources().head_cpus, ray_params.get_tune_resources().bundles, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581ac17b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST 1\n",
    "\n",
    "import xgboost_ray as xr\n",
    "import ray\n",
    "from xgboost_ray import RayDMatrix, RayParams, train\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "ray.init()\n",
    "\n",
    "train_x, train_y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# NEED TO ARTIFICALLY BOOST MODEL SIZE DUE TO TOO QUICK TRAINING \n",
    "for i in range(6): # 6 = x64  # on AWS c5.xLarge 6 is MAX\n",
    "    train_x = np.vstack([train_x,train_x])\n",
    "    train_x = np.hstack([train_x,train_x])    \n",
    "    train_y = np.hstack([train_y, train_y])\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "\n",
    "train_set = RayDMatrix(train_x, train_y)\n",
    "\n",
    "#print(train_x.shape, type(train_x))\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "bst = train({\"objective\": \"binary:logistic\",\n",
    "             \"eval_metric\": [\"logloss\", \"error\"],},\n",
    "            train_set,\n",
    "            evals_result=evals_result,\n",
    "            evals=[(train_set, \"train\")],\n",
    "            verbose_eval=True,\n",
    "            ray_params=RayParams(num_actors = 1, # Number of Remote Actors\n",
    "                                 gpus_per_actor = 1,\n",
    "                                 cpus_per_actor = 1))\n",
    "\n",
    "print(\"TRAIN TIME\", time.time() - start_time)\n",
    "\n",
    "bst.save_model(\"model.xgb\")\n",
    "\n",
    "print(\"Final training error: {:.4f}, {:.4f}\".format(\n",
    "    evals_result[\"train\"][\"error\"][-1],\n",
    "    evals_result[\"train\"][\"logloss\"][-1]))\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316eb0e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### M1\n",
    "\n",
    "TO utilize parallelism you need to add ray.init(num_cpu = XXX) or just ray.init()\n",
    "# num_actors=1, cpu_per_actor = Not Specified <-- BEST (no need to specify the cpu_per_actor as RAY utilizes \n",
    "                                                        multithreading and thus uses all avalube CPUs as dfault)\n",
    "\n",
    "WITH ray.init() # NICE SPEEDUP \n",
    "TRAIN TIME 2.66068696975708\n",
    "Final training error: 0.0000, 0.0253\n",
    "\n",
    "\n",
    "NO ray.init()\n",
    "# num_actors=1, cpu_per_actor = 1\n",
    "TRAIN TIME 6.129012107849121\n",
    "Final training error: 0.0000\n",
    "    \n",
    "# num_actors=1, cpu_per_actor = 10\n",
    "TRAIN TIME 6.171634197235107\n",
    "Final training error: 0.0000\n",
    "    \n",
    "# num_actors=2, cpu_per_actor = 1\n",
    "TRAIN TIME 7.246088027954102\n",
    "Final training error: 0.0000\n",
    "    \n",
    "# num_actors=2, cpu_per_actor = 5\n",
    "TRAIN TIME 6.13xxxxxxxxxxxxx\n",
    "Final training error: 0.0000 \n",
    "    \n",
    "# num_actors=3, cpu_per_actor = 3\n",
    "TRAIN TIME 7.325688123703003\n",
    "Final training error: 0.0000\n",
    "    \n",
    "# num_actors=10, cpu_per_actor = 1\n",
    "TRAIN TIME 10.49834394454956\n",
    "Final training error: 0.0000\n",
    "\n",
    "\n",
    "\n",
    "### AWS c5.xlarge -- 4 CPU, 0 GPU, NO ray.init()\n",
    "-- HERE CPU per actor works as expected ALTHOUGH not big boost\n",
    "\n",
    "# num_actors=1, cpu_per_actor = 1 - range(4)\n",
    "TRAIN TIME 11.816567659378052\n",
    "Final training error: 0.0000, 0.0253\n",
    "\n",
    "# num_actors=1, cpu_per_actor = 1 - range(6)\n",
    "TRAIN TIME 63.819782972335815\n",
    "Final training error: 0.0000, 0.0238\n",
    "\n",
    "\n",
    "# num_actors=1, cpu_per_actor = 4 - range(4)\n",
    "TRAIN TIME 9.398769855499268\n",
    "Final training error: 0.0000, 0.0253\n",
    "\n",
    "# num_actors=1, cpu_per_actor = 4 - range(6)\n",
    "TRAIN TIME 54.842820167541504\n",
    "Final training error: 0.0000, 0.0238\n",
    "\n",
    "\n",
    "\n",
    "### AWS p3.2xlarge -- 4 CPU, 1 GPU, NO ray.init()\n",
    "\n",
    "# num_actors=1, cpu_per_actor = 1 - range(6)\n",
    "TRAIN TIME 36.15526247024536\n",
    "Final training error: 0.0000, 0.0238\n",
    "\n",
    "# num_actors=1, cpu_per_actor = 4 - range(6)\n",
    "TRAIN TIME 32.72776222229004\n",
    "Final training error: 0.0000, 0.0238\n",
    "\n",
    "# num_actors=1, cpu_per_actor = 1, GPU = 1 - range(6)\n",
    "TRAIN TIME 35.24531555175781\n",
    "Final training error: 0.0000, 0.0238"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9950d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2\n",
    "\n",
    "# NOT SURE IF I RECOREDED RESULTS -- PROBABLY SEARCH FOR SPECIFICATION THAT RESULTS IN VISIBLE PRALLELIZATION\n",
    "from xgboost_ray import RayDMatrix, RayParams, predict\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import xgboost as xgb\n",
    "\n",
    "train_x, train_y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "\"\"\"\n",
    "# NEED TO ARTIFICALLY BOOST MODEL SIZE DUE TO TOO QUICK TRAINING \n",
    "for i in range(4): # 6 = x64\n",
    "    train_x = np.vstack([train_x,train_x])\n",
    "    train_x = np.hstack([train_x,train_x])    \n",
    "    train_y = np.hstack([train_y, train_y])\n",
    "\"\"\"\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "\n",
    "dpred = RayDMatrix(train_x, train_y)\n",
    "\n",
    "bst = xgb.Booster(model_file=\"model.xgb\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "pred_ray = predict(bst, dpred, ray_params=RayParams(num_actors=4))\n",
    "\n",
    "print(\"TRAIN TIME\", time.time() - start_time)\n",
    "\n",
    "print(pred_ray)\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e408d88e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST 3\n",
    "\n",
    "# DATA GENERATION FOR THE NEXT CELL\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import ray\n",
    "\n",
    "# define dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=20000, #20000\n",
    "    n_features=1000,\n",
    "    n_informative=50,\n",
    "    n_redundant=0,\n",
    "    random_state=1)\n",
    "# summarize the dataset\n",
    "print(X.shape, y.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.50, random_state=1)\n",
    "\n",
    "from xgboost_ray import RayDMatrix, RayParams, train\n",
    "\n",
    "train_set = RayDMatrix(X_train, y_train)\n",
    "eval_set = RayDMatrix(X_test, y_test)\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d449567",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ray.init(num_cpus = 2, num_gpus = 1) #, ignore_reinit_error=True)\n",
    "\n",
    "evals_result = {}\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "bst = train({\"objective\": \"binary:logistic\",\n",
    "             \"eval_metric\": [\"logloss\", \"error\"],\n",
    "             \"tree_method\": \"gpu_hist\",},\n",
    "            train_set,\n",
    "            num_boost_round=10,\n",
    "            evals_result=evals_result,\n",
    "            evals=[(train_set, \"train\"), \n",
    "                   (eval_set, \"eval\")],\n",
    "            verbose_eval=True,\n",
    "            ray_params=RayParams(num_actors=1,\n",
    "                                 gpus_per_actor=1,\n",
    "                                 cpus_per_actor=1,  # Divide evenly across actors per machine\n",
    "                                ))\n",
    "\n",
    "print(\"TRAIN TIME\", time.time() - start_time)\n",
    "\n",
    "bst.save_model(\"model.xgb\")\n",
    "\n",
    "print(\"Final training error: {:.4f}\".format(evals_result[\"train\"][\"error\"][-1]))\n",
    "print(\"Final validation error: {:.4f}\".format(evals_result[\"eval\"][\"error\"][-1]))\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8f6b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### M1\n",
    "#num_actors = 1/cpus_per_actor = 1\n",
    "TRAIN TIME 7.220772981643677\n",
    "Final training error: 0.0723\n",
    "Final validation error: 0.1279\n",
    "    \n",
    "#2/1\n",
    "TRAIN TIME 7.784131050109863\n",
    "Final training error: 0.0730\n",
    "Final validation error: 0.1317    \n",
    "    \n",
    "#10/1 -- Failed\n",
    "\n",
    "#1/1\n",
    "TRAIN TIME 9.191377878189087\n",
    "Final training error: 0.0568\n",
    "Final validation error: 0.1448\n",
    "    \n",
    "#1/2\n",
    "TRAIN TIME 8.250745058059692\n",
    "Final training error: 0.0568\n",
    "Final validation error: 0.1448\n",
    "    \n",
    "#1/4\n",
    "TRAIN TIME 8.249265909194946\n",
    "Final training error: 0.0568\n",
    "Final validation error: 0.1448  \n",
    "    \n",
    "#1/10    \n",
    "TRAIN TIME 8.197720766067505\n",
    "Final training error: 0.0568\n",
    "Final validation error: 0.1448\n",
    "\n",
    "\n",
    "\n",
    "### AWS c5.xlarge -- FOr this task PARALLELIZATION DOES NOTG WORK?!?!?!?\n",
    "\n",
    "#1/1, n_samples = 20000\n",
    "TRAIN TIME 17.756388425827026\n",
    "Final training error: 0.0568\n",
    "Final validation error: 0.1448\n",
    "\n",
    "#1/4, 20000\n",
    "TRAIN TIME 18.528228521347046\n",
    "Final training error: 0.0568\n",
    "Final validation error: 0.1448\n",
    "\n",
    "#4/1, 20000 -- FAILED DUE TO MEMORY \n",
    "TRAIN TIME 18.528228521347046\n",
    "Final training error: 0.0568\n",
    "Final validation error: 0.1448\n",
    "\n",
    "#4/1, 10000\n",
    "TRAIN TIME 31.477298498153687\n",
    "Final training error: 0.0342\n",
    "Final validation error: 0.1736\n",
    "\n",
    "#1/1, 10000\n",
    "TRAIN TIME 17.673475980758667\n",
    "Final training error: 0.0302\n",
    "Final validation error: 0.1650\n",
    "\n",
    "\n",
    "\n",
    "### AWS p3.2xlarge\n",
    "\n",
    "#1/1, 10000\n",
    "TRAIN TIME 19.729962587356567\n",
    "Final training error: 0.0302\n",
    "Final validation error: 0.1650\n",
    "\n",
    "#1/1, 20000\n",
    "TRAIN TIME 21.833100080490112\n",
    "Final training error: 0.0568\n",
    "Final validation error: 0.1448\n",
    "\n",
    "#1/8, 20000\n",
    "TRAIN TIME 18.813719272613525\n",
    "Final training error: 0.0568\n",
    "Final validation error: 0.1448\n",
    "\n",
    "#1/8/1 GPU, 20000\n",
    "TRAIN TIME 9.853558540344238\n",
    "Final training error: 0.0576\n",
    "Final validation error: 0.1478\n",
    "\n",
    "#1/1/1 GPU, 20000\n",
    "TRAIN TIME 9.803324222564697\n",
    "Final training error: 0.0576\n",
    "Final validation error: 0.1478"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094486a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST 4\n",
    "\n",
    "from xgboost_ray import RayXGBClassifier, RayParams\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "\n",
    "seed = 42\n",
    "\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    train_size=0.25, \n",
    "                                                    random_state=42)\n",
    "\n",
    "clf = RayXGBClassifier(n_jobs=1,  # In XGBoost-Ray, n_jobs sets the number of actors\n",
    "                       random_state=seed\n",
    "                      )\n",
    "\n",
    "# scikit-learn API will automatically convert the data\n",
    "# to RayDMatrix format as needed.\n",
    "# You can also pass X as a RayDMatrix, in which case\n",
    "# y will be ignored.\n",
    "\"\"\"\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"TRAIN TIME\", time.time() - start_time)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "pred_ray = clf.predict(X_test)\n",
    "print(\"PREDICTION\", pred_ray)\n",
    "\n",
    "pred_proba_ray = clf.predict_proba(X_test)\n",
    "print(\"PREDICTION_PRBABILITY\", pred_proba_ray)\n",
    "\"\"\"\n",
    "\n",
    "# It is also possible to pass a RayParams object\n",
    "# to fit/predict/predict_proba methods - will override\n",
    "# n_jobs set during initialization\n",
    "\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train, ray_params=RayParams(num_actors=1))\n",
    "print(\"TRAIN TIME\", time.time() - start_time)\n",
    "\n",
    "\"\"\"\n",
    "pred_ray = clf.predict(X_test, ray_params=RayParams(num_actors=2))\n",
    "print(\"PREDICTION\", pred_ray)\n",
    "\"\"\"\n",
    "ray.shutdown()\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb014611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIMING IS CONTRARY TO EXPECTATIONS BUT WE CAN SEE VISIBLE IMPACT OF actgors/jobs on it.\n",
    "\n",
    "n_jobs = 1\n",
    "TRAIN TIME 3.673287868499756 -- it was also as. much as 7.xxx\n",
    "\n",
    "n_jobs = 2\n",
    "TRAIN TIME 7.201511859893799\n",
    "\n",
    "n_jobs = 4\n",
    "TRAIN TIME 7.115774154663086\n",
    "\n",
    "n_jobs = 10\n",
    "TRAIN TIME 8.667701005935669\n",
    "\n",
    "\n",
    "num_actors = 1\n",
    "TRAIN TIME 7.2665088176727295\n",
    "\n",
    "num_actors = 2\n",
    "TRAIN TIME 7.180140018463135\n",
    "\n",
    "num_actors = 4\n",
    "TRAIN TIME 6.277163028717041\n",
    "\n",
    "num_actors = 10\n",
    "TRAIN TIME 8.599561929702759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbc2e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TEST 5\n",
    "\n",
    "import ray\n",
    "from xgboost_ray import RayDMatrix, RayParams, train\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "num_actors = 1 # number of acoters per tune_job \n",
    "                #(i.e. if you have 10 CPU cores you will be able to run run only 5 Tune Jopbs in parallel)\n",
    "num_cpus_per_actor = 1\n",
    "\n",
    "ray_params = RayParams(num_actors=num_actors,\n",
    "                       gpus_per_actor=1,\n",
    "                       cpus_per_actor=num_cpus_per_actor)\n",
    "\n",
    "def train_model(config):\n",
    "    train_x, train_y = load_breast_cancer(return_X_y=True)\n",
    "    train_set = RayDMatrix(train_x, train_y)\n",
    "\n",
    "    evals_result = {}\n",
    "    \n",
    "    bst = train(params=config,\n",
    "                dtrain=train_set,\n",
    "                evals_result=evals_result,\n",
    "                evals=[(train_set, \"train\")],\n",
    "                verbose_eval=False,\n",
    "                ray_params=ray_params)\n",
    "    \n",
    "    bst.save_model(\"model.xgb\")\n",
    "\n",
    "from ray import tune\n",
    "\n",
    "# Specify the hyperparameter search space.\n",
    "config = {\"tree_method\": \"approx\",\n",
    "          \"objective\": \"binary:logistic\",\n",
    "          \"eval_metric\": [\"logloss\", \"error\"],\n",
    "          \"eta\": tune.loguniform(1e-4, 1e-1),\n",
    "          \"subsample\": tune.uniform(0.5, 1.0),\n",
    "          \"max_depth\": tune.randint(1, 9),\n",
    "          \"tree_method\": \"gpu_hist\",\n",
    "         }\n",
    "\n",
    "# Make sure to use the `get_tune_resources` method to set the `resources_per_trial`\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "analysis = tune.run(train_model,\n",
    "                    config=config,\n",
    "                    metric=\"train-error\",\n",
    "                    mode=\"min\",\n",
    "                    num_samples=20,\n",
    "                    resources_per_trial=ray_params.get_tune_resources())\n",
    "\n",
    "print(\"TRAIN TIME\", time.time() - start_time)\n",
    "\n",
    "display(\"Best hyperparameters\", analysis.best_config)\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2bdab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actors: # number of acoters per tune_job \n",
    "            # (i.e. if you have 10 CPU cores and value is 2 you will be able to run run only 5 Tune Jobs in parallel)\n",
    "            # (i.e. for value 1 you will be able to run run 10 Tune Jobs in parallel)\n",
    "\n",
    "            \n",
    "### M1\n",
    "\n",
    "num_actors = 1, num_CPUs = 1, num_samples_4, ray_params.get_tune_resources()\n",
    "TRAIN TIME 12.8049880027771\n",
    "\n",
    "num_actors = 1, num_CPUs = 1, num_samples_10, ray_params.get_tune_resources()\n",
    "TRAIN TIME 19.9549880027771\n",
    "\n",
    "num_actors = 1, num_CPUs = 1, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 34.25311517715454\n",
    "\n",
    "num_actors = 2, num_CPUs = 1, num_samples_4, ray_params.get_tune_resources()\n",
    "TRAIN TIME 13.603897094726562\n",
    "\n",
    "num_actors = 2, num_CPUs = 1, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 39.90977382659912\n",
    "\n",
    "num_actors = 10, num_CPUs = 1, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 138.29227209091187\n",
    "\n",
    "num_actors = 1, num_CPUs = 2, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 32.94475817680359\n",
    "\n",
    "num_actors = 1, num_CPUs = 4, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 47.004079818725586\n",
    "\n",
    "num_actors = 1, num_CPUs = 10, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 70.54199194908142\n",
    "\n",
    "\n",
    "\n",
    "### AWS c5.xlarge -- 4CPUs\n",
    "\n",
    "num_actors = 1, num_CPUs = 1, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 68.40848469734192\n",
    "\n",
    "num_actors = 1, num_CPUs = 4, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 102.29872679710388\n",
    "\n",
    "num_actors = 4, num_CPUs = 1, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 162.59184432029724\n",
    "\n",
    "\n",
    "\n",
    "### AWS p5.2xlarge -- 8CPUs, 1 GPU\n",
    "\n",
    "num_actors = 1, num_CPUs = 1, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 49.652567625045776\n",
    "\n",
    "num_actors = 1, num_CPUs = 1, num_GPUs = 1, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 179.6664788722992\n",
    "\n",
    "num_actors = 1, num_CPUs = 8, num_GPUs = 1, num_samples_20, ray_params.get_tune_resources()\n",
    "TRAIN TIME 184.71277117729187\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a5225-4ba0-4b17-8818-90b91c562036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xgboost_ray as xr\n",
    "import ray\n",
    "from xgboost_ray import RayDMatrix, RayParams, train\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import time, os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "parallel_num = 1\n",
    "#ray.init(num_cpus = parallel_num)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "train_x, train_y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "for i in range(15): # 6 = x64 #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    train_x = np.vstack([train_x,train_x])\n",
    "    #print(\"1\", train_x.shape)\n",
    "    if i <= -1: #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        train_x = np.hstack([train_x,train_x])\n",
    "        #print(\"2\", train_x.shape)    \n",
    "    train_y = np.hstack([train_y, train_y])\n",
    "    print(\"3\", i, train_y.shape)    \n",
    "    \n",
    "print(train_x.shape, train_y.shape)\n",
    "\n",
    "train_data = np.concatenate([train_x, train_y.reshape([-1,1])], axis = 1)\n",
    "print(train_data.shape)\n",
    "\n",
    "train_data_DF = pd.DataFrame(train_data)\n",
    "\n",
    "col_names = train_data_DF.columns\n",
    "print(\"col_names\", col_names)\n",
    "\n",
    "col_names_str = [str(i) for i in col_names]\n",
    "col_names_str[-1] = \"Target\"\n",
    "print(col_names_str)\n",
    "\n",
    "train_data_DF.columns = col_names_str\n",
    "                 \n",
    "train_data_DF.to_parquet('train_data.parquet', \n",
    "                         #compression = 'brotli',\n",
    "                         index = False )\n",
    "\n",
    "# Parquet file takes up little space \n",
    "print(os.path.getsize('train_data.parquet'))\n",
    "\n",
    "def get_parquet_files(path, size = 10):\n",
    "    files = sorted(glob.glob(path))\n",
    "    while size > len(files):\n",
    "        files = files + files\n",
    "    files = files[0:size]\n",
    "    return files\n",
    "\n",
    "data_path = f\"./*.parquet\"\n",
    "\n",
    "data_files_1 = get_parquet_files(data_path, size = 1) #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "data_files_2 = get_parquet_files(data_path, size = 2)\n",
    "data_files_3 = get_parquet_files(data_path, size = 3)\n",
    "data_files_4 = get_parquet_files(data_path, size = 4)\n",
    "data_files_5 = get_parquet_files(data_path, size = 5)\n",
    "data_files_10 = get_parquet_files(data_path, size = 10)\n",
    "data_files_13 = get_parquet_files(data_path, size = 13)\n",
    "data_files_16 = get_parquet_files(data_path, size = 16)\n",
    "data_files_100 = get_parquet_files(data_path, size = 100)\n",
    "\n",
    "xgboost_params = {\"tree_method\": \"gpu_hist\", #\"approx\",\n",
    "                 \"objective\": \"binary:logistic\",\n",
    "                 \"eval_metric\": [\"logloss\", \"error\"]}\n",
    "\n",
    "def train_xgboost(config, files, ray_params, progress_bar=False):\n",
    "    target_column = \"Target\"\n",
    "    \n",
    "    train_set = RayDMatrix(files, target_column, distributed=False) #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    test_set = RayDMatrix(files[0], target_column)\n",
    "    \n",
    "    evals_results = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    bst = train(params = config,\n",
    "               dtrain = train_set,\n",
    "               #evals = [(test_set, \"eval\")],\n",
    "               #evals_results = evals_results,\n",
    "               #verbose_eval = False,\n",
    "               num_boost_round = 100, #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "               #callbacks = [TqdmCallback(10)] if progress_bar else [],\n",
    "               ray_params = ray_params)\n",
    "    print(\"TRAIN TIME\", time.time() - start_time)\n",
    "    \n",
    "    return bst\n",
    "\n",
    "bst = train_xgboost(xgboost_params, data_files_1, RayParams(num_actors = 1, cpus_per_actor = 16)) #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c574a1-52e3-480e-8fb3-0f3df35ad476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# M1\n",
    "\n",
    "range(9), <=3, num_boost_round = 1, data_files_1, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "\n",
    "range(8), <=3, num_boost_round = 1, data_files_2, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 5.729285955429077\n",
    "\n",
    "range(8), <=3, num_boost_round = 10, data_files_2, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 11.767457962036133\n",
    "\n",
    "range(8), <=3, num_boost_round = 10, data_files_2, RayParams(num_actors = 2, cpus_per_actor = 1))\n",
    "TRAIN TIME 10.743774890899658\n",
    "\n",
    "range(8), <=3, num_boost_round = 10, data_files_3, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 17.33546018600464\n",
    "\n",
    "range(8), <=3, num_boost_round = 10, data_files_3, RayParams(num_actors = 2, cpus_per_actor = 1))\n",
    "TRAIN TIME 14.88601303100586\n",
    "\n",
    "range(8), <=3, num_boost_round = 10, data_files_3, RayParams(num_actors = 3, cpus_per_actor = 1))\n",
    "TRAIN TIME 14.014294862747192\n",
    "\n",
    "range(8), <=3, num_boost_round = 10, data_files_3, RayParams(num_actors = 3, cpus_per_actor = 3))\n",
    "TRAIN TIME 12.830633401870728\n",
    "\n",
    "range(7), <=4, num_boost_round = 10, data_files_3, RayParams(num_actors = 3, cpus_per_actor = 3))\n",
    "TRAIN TIME 14.921172857284546\n",
    "\n",
    "range(7), <=4, num_boost_round = 10, data_files_3, RayParams(num_actors = 3, cpus_per_actor = 1))\n",
    "TRAIN TIME 15.9930899143219\n",
    "\n",
    "range(7), <=4, num_boost_round = 10, data_files_3, RayParams(num_actors = 2, cpus_per_actor = 1))\n",
    "TRAIN TIME 15.79627275466919\n",
    "\n",
    "range(7), <=4, num_boost_round = 10, data_files_3, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 15.83592414855957\n",
    "\n",
    "range(9), <=2, num_boost_round = 10, data_files_3, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 16.924689769744873\n",
    "\n",
    "range(9), <=2, num_boost_round = 10, data_files_3, RayParams(num_actors = 3, cpus_per_actor = 3))\n",
    "TRAIN TIME 12.809518098831177\n",
    "\n",
    "range(12), <=-1, num_boost_round = 10, data_files_3, RayParams(num_actors = 3, cpus_per_actor = 3))\n",
    "TRAIN TIME 14.860911130905151\n",
    "\n",
    "range(12), <=-1, num_boost_round = 10, data_files_3, RayParams(num_actors = 3, cpus_per_actor = 3))\n",
    "TRAIN TIME 19.94079089164734\n",
    "\n",
    "range(10), <=-1, num_boost_round = 10, data_files_10, RayParams(num_actors = 10, cpus_per_actor = 1))\n",
    "TRAIN TIME 11.22438383102417\n",
    "\n",
    "range(10), <=-1, num_boost_round = 10, data_files_10, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 16.732402801513672\n",
    "\n",
    "range(10), <=-1, num_boost_round = 10, data_files_13, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 22.021054983139038\n",
    "\n",
    "range(10), <=-1, num_boost_round = 10, data_files_13, RayParams(num_actors = 10, cpus_per_actor = 1))\n",
    "TRAIN TIME 14.368117094039917\n",
    "\n",
    "range(10), <=-1, num_boost_round = 10, data_files_13, RayParams(num_actors = 5, cpus_per_actor = 1))\n",
    "TRAIN TIME 14.076416015625\n",
    "\n",
    "range(10), <=-1, num_boost_round = 10, data_files_13, RayParams(num_actors = 5, cpus_per_actor = 2))\n",
    "TRAIN TIME 14.095832109451294\n",
    "\n",
    "range(10), <=-1, num_boost_round = 10, data_files_13, RayParams(num_actors = 2, cpus_per_actor = 1))\n",
    "TRAIN TIME 16.76328182220459\n",
    "\n",
    "range(10), <=-1, num_boost_round = 10, data_files_13, RayParams(num_actors = 2, cpus_per_actor = 5))\n",
    "TRAIN TIME 14.764201879501343\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 10, cpus_per_actor = 1))\n",
    "TRAIN TIME 84.82407999038696\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 153.25520396232605\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 2, cpus_per_actor = 1))\n",
    "TRAIN TIME 117.27706384658813\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 2, cpus_per_actor = 5))\n",
    "TRAIN TIME 103.20557403564453\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 5, cpus_per_actor = 2))\n",
    "TRAIN TIME 86.4503219127655\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 5, cpus_per_actor = 1))\n",
    "TRAIN TIME 89.2873420715332\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 4, cpus_per_actor = 2))\n",
    "TRAIN TIME 88.2671947479248\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 3, cpus_per_actor = 2))\n",
    "TRAIN TIME 94.20934391021729\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 3, cpus_per_actor = 3))\n",
    "TRAIN TIME 93.2304048538208\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 6, cpus_per_actor = 1))\n",
    "TRAIN TIME 87.40560507774353\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 7, cpus_per_actor = 1))\n",
    "TRAIN TIME 83.48936223983765\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 8, cpus_per_actor = 1))\n",
    "TRAIN TIME 82.53302001953125\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_13, RayParams(num_actors = 9, cpus_per_actor = 1))\n",
    "TRAIN TIME 84.2537407875061\n",
    "\n",
    "\n",
    "### AWS c5.xlarge -- 4CPUs\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_4, RayParams(num_actors = 4, cpus_per_actor = 1))\n",
    "TRAIN TIME 225.0817904472351\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_5, RayParams(num_actors = 4, cpus_per_actor = 1))\n",
    "TRAIN TIME 298.0817904472351\n",
    "\n",
    "range(10), <=-1, num_boost_round = 100, data_files_6, RayParams(num_actors = 4, cpus_per_actor = 1))\n",
    "TRAIN TIME 350.51361751556396\n",
    "\n",
    "range(11), <=-1, num_boost_round = 100, data_files_4, RayParams(num_actors = 4, cpus_per_actor = 1))\n",
    "TRAIN TIME 450.78320503234863\n",
    "\n",
    "range(11), <=-1, num_boost_round = 100, data_files_4, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 475.6997878551483\n",
    "\n",
    "range(11), <=-1, num_boost_round = 100, data_files_4, RayParams(num_actors = 1, cpus_per_actor = 4))\n",
    "TRAIN TIME 429.6034173965454\n",
    "\n",
    "\n",
    "\n",
    "### AWS c5.4xlarge -- 16CPUs\n",
    "range(11), <=-1, num_boost_round = 100, data_files_16, RayParams(num_actors = 16, cpus_per_actor = 1))\n",
    "TRAIN TIME 536.9499912261963\n",
    "\n",
    "range(11), <=-1, num_boost_round = 100, data_files_16, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 770.5534617900848\n",
    "\n",
    "range(11), <=-1, num_boost_round = 100, data_files_16, RayParams(num_actors = 1, cpus_per_actor = 16))\n",
    "TRAIN TIME 579.2549285888672\n",
    "\n",
    "range(11), <=-1, num_boost_round = 100, data_files_1, RayParams(num_actors = 1, cpus_per_actor = 16))\n",
    "TRAIN TIME 33.595520973205566\n",
    "\n",
    "range(11), <=-1, num_boost_round = 100, data_files_1, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 47.637054204940796\n",
    "\n",
    "range(11), <=-1, num_boost_round = 100, data_files_16, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 535.7039926052094\n",
    "\n",
    "\n",
    "range(15), <=-1, num_boost_round = 1, data_files_1, distributed=False, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 34.72002863883972\n",
    "\n",
    "range(15), <=-1, num_boost_round = 1, data_files_1, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 35.39907503128052\n",
    "\n",
    "range(15), <=-1, num_boost_round = 1, data_files_1, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 16))\n",
    "TRAIN TIME 33.274383783340454\n",
    "\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_1, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 16)) # BEST\n",
    "TRAIN TIME 580.9696714878082\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_1, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 769.5239207744598\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_1, distributed=False, RayParams(num_actors = 1, cpus_per_actor = 1))\n",
    "TRAIN TIME 770.5029118061066\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_1, distributed=False, RayParams(num_actors = 1, cpus_per_actor = 16))\n",
    "TRAIN TIME 592.8946852684021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06e812-68e7-4a0c-b532-442d1e52ec26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPUs\n",
    "\n",
    "import xgboost_ray as xr\n",
    "import ray\n",
    "from xgboost_ray import RayDMatrix, RayParams, train\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import time, os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "parallel_num = 1\n",
    "#ray.init(num_cpus = parallel_num)\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "train_x, train_y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "for i in range(15): # 6 = x64 #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    train_x = np.vstack([train_x,train_x])\n",
    "    #print(\"1\", train_x.shape)\n",
    "    if i <= -1: #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "        train_x = np.hstack([train_x,train_x])\n",
    "        #print(\"2\", train_x.shape)    \n",
    "    train_y = np.hstack([train_y, train_y])\n",
    "    print(\"3\", i, train_y.shape)    \n",
    "    \n",
    "print(train_x.shape, train_y.shape)\n",
    "\n",
    "train_data = np.concatenate([train_x, train_y.reshape([-1,1])], axis = 1)\n",
    "print(train_data.shape)\n",
    "\n",
    "train_data_DF = pd.DataFrame(train_data)\n",
    "\n",
    "col_names = train_data_DF.columns\n",
    "print(\"col_names\", col_names)\n",
    "\n",
    "col_names_str = [str(i) for i in col_names]\n",
    "col_names_str[-1] = \"Target\"\n",
    "print(col_names_str)\n",
    "\n",
    "train_data_DF.columns = col_names_str\n",
    "                 \n",
    "train_data_DF.to_parquet('train_data.parquet', \n",
    "                         #compression = 'brotli',\n",
    "                         index = False )\n",
    "\n",
    "# Parquet file takes up little space \n",
    "print(os.path.getsize('train_data.parquet'))\n",
    "\n",
    "def get_parquet_files(path, size = 10):\n",
    "    files = sorted(glob.glob(path))\n",
    "    while size > len(files):\n",
    "        files = files + files\n",
    "    files = files[0:size]\n",
    "    return files\n",
    "\n",
    "data_path = f\"./*.parquet\"\n",
    "\n",
    "data_files_1 = get_parquet_files(data_path, size = 1) #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "data_files_2 = get_parquet_files(data_path, size = 2)\n",
    "data_files_3 = get_parquet_files(data_path, size = 3)\n",
    "data_files_4 = get_parquet_files(data_path, size = 4)\n",
    "data_files_5 = get_parquet_files(data_path, size = 5)\n",
    "data_files_8 = get_parquet_files(data_path, size = 8)\n",
    "data_files_10 = get_parquet_files(data_path, size = 10)\n",
    "data_files_13 = get_parquet_files(data_path, size = 13)\n",
    "data_files_16 = get_parquet_files(data_path, size = 16)\n",
    "data_files_100 = get_parquet_files(data_path, size = 100)\n",
    "\n",
    "xgboost_params = {\"tree_method\": \"gpu_hist\", #\"approx\",\n",
    "                 \"objective\": \"binary:logistic\",\n",
    "                 \"eval_metric\": [\"logloss\", \"error\"]}\n",
    "\n",
    "def train_xgboost(config, files, ray_params, progress_bar=False):\n",
    "    target_column = \"Target\"\n",
    "    \n",
    "    train_set = RayDMatrix(files, target_column, distributed=True) #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "    test_set = RayDMatrix(files[0], target_column)\n",
    "    \n",
    "    evals_results = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    bst = train(params = config,\n",
    "               dtrain = train_set,\n",
    "               #evals = [(test_set, \"eval\")],\n",
    "               #evals_results = evals_results,\n",
    "               #verbose_eval = False,\n",
    "               num_boost_round = 100, #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "               #callbacks = [TqdmCallback(10)] if progress_bar else [],\n",
    "               ray_params = ray_params)\n",
    "    print(\"TRAIN TIME\", time.time() - start_time)\n",
    "    \n",
    "    return bst\n",
    "\n",
    "bst = train_xgboost(xgboost_params, \n",
    "                    data_files_16, \n",
    "                    RayParams(num_actors = 1, \n",
    "                              cpus_per_actor = 8, \n",
    "                              gpus_per_actor = 1)) #@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "ray.shutdown()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a14ee4-4fa6-4e21-8bf3-6f29e70e0056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p3.2xlarge  == 1 GPU\n",
    "- range() can go as high up as 16. but i keep 15 for broader comparison\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_1, distributed=False, RayParams(num_actors = 1, cpus_per_actor = 1, gpus_per_actor = 1))\n",
    "TRAIN TIME 28.9706072807312, TRAIN TIME 28.95785355567932 !!!!\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_1, distributed=False, RayParams(num_actors = 1, cpus_per_actor = 8, gpus_per_actor = 1))\n",
    "TRAIN TIME 30.178287029266357\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_1, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 1, gpus_per_actor = 1))\n",
    "TRAIN TIME 30.19754147529602\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_1, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 8, gpus_per_actor = 1))\n",
    "TRAIN TIME 30.209967136383057\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_2, distributed=False, RayParams(num_actors = 1, cpus_per_actor = 1, gpus_per_actor = 1))\n",
    "TRAIN TIME 55.98624062538147, TRAIN TIME 56.40909028053284\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_2, distributed=False, RayParams(num_actors = 1, cpus_per_actor = 8, gpus_per_actor = 1))\n",
    "TRAIN TIME 57.961164712905884\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_2, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 8, gpus_per_actor = 1))\n",
    "TRAIN TIME 55.60385990142822\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_2, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 1, gpus_per_actor = 1))\n",
    "TRAIN TIME 54.5383083820343\n",
    "\n",
    "\n",
    "# p3.8xlarge  == 4 GPU\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_1, distributed=False, RayParams(num_actors = 1, cpus_per_actor = 1, gpus_per_actor = 1))\n",
    "TRAIN TIME 28.587210178375244\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_4, distributed=False, RayParams(num_actors = 4, cpus_per_actor = 1, gpus_per_actor = 1))\n",
    "TRAIN TIME 73.05642509460449\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_4, distributed=False, RayParams(num_actors = 4, cpus_per_actor = 4, gpus_per_actor = 1))\n",
    "TRAIN TIME 72.00212550163269\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_4, distributed=True, RayParams(num_actors = 4, cpus_per_actor = 4, gpus_per_actor = 1))\n",
    "TRAIN TIME 33.4779314994812\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_4, distributed=True, RayParams(num_actors = 4, cpus_per_actor = 1, gpus_per_actor = 1))\n",
    "TRAIN TIME 34.28415489196777\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_4, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 1, gpus_per_actor = 1))\n",
    "TRAIN TIME 97.59232831001282\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_4, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 8, gpus_per_actor = 1))\n",
    "TRAIN TIME 98.0595326423645\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_8, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 8, gpus_per_actor = 1))\n",
    "TRAIN TIME 197.67186951637268\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_8, distributed=True, RayParams(num_actors = 4, cpus_per_actor = 2, gpus_per_actor = 1))\n",
    "TRAIN TIME 63.57376527786255\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_8, distributed=True, RayParams(num_actors = 1, cpus_per_actor = 8, gpus_per_actor = 1))\n",
    "\n",
    "range(15), <=-1, num_boost_round = 100, data_files_8, distributed=True, RayParams(num_actors = 4, cpus_per_actor = 2, gpus_per_actor = 1))\n",
    "TRAIN TIME 136.5337245464325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596a6798-258b-4b48-8d75-55a666e74d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"8 Shards\", 63.57376527786255/197.67186951637268, \"4 Shardes\", 33.4779314994812/97.59232831001282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d3179-54d1-40d6-89a8-15af6cd9c108",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
