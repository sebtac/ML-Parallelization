{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df910e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### DRAFT ##################\n",
    "Implementation of GYM's Multi_ENV_Per_Actor parameter with MLxE architecture.\n",
    "\n",
    "- Unvectorized Implementation with RAY works\n",
    "- But the Vectorized version needs need an upate to VECTORIZED GYM ENVIRONMENTS to the RESET method to allow it a proper RESET at the end of the episode\n",
    "    - See StackOverflow issue:\n",
    "        - https://stackoverflow.com/questions/75551863/vectorized-gym-environments-how-to-block-automatic-environment-reset-on-done-t\n",
    "        \n",
    "    - Possibel ways to fix it (TBC):\n",
    "        - change the guts of the GYM package.\n",
    "        - checkout gym3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3509a9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.output_result { max-width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-26 12:57:37,113\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(Memorizer pid=4485)\u001b[0m 2023-02-26 12:57:40.615139: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "\u001b[2m\u001b[36m(Memorizer pid=4485)\u001b[0m 2023-02-26 12:57:40.615280: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m 2023-02-26 12:57:40.613277: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m 2023-02-26 12:57:40.613408: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "\u001b[2m\u001b[36m(pid=4471)\u001b[0m   import imp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Memorizer pid=4485)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train pid=4471)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:47: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(train pid=4471)\u001b[0m   min_version = LooseVersion(INCLUSIVE_MIN_TF_VERSION)\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _descriptor.EnumValueDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _DATATYPE = _descriptor.EnumDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _TENSORPROTO = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _descriptor.FieldDescriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   _ATTRVALUE_LISTVALUE = _descriptor.Descriptor(\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "\u001b[2m\u001b[36m(pid=4486)\u001b[0m   import imp\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m 2023-02-26 12:57:43.603720: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m 2023-02-26 12:57:43.603834: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m /Users/sebtac/miniforge3/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:47: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m   min_version = LooseVersion(INCLUSIVE_MIN_TF_VERSION)\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m /var/folders/h4/d_xxpqw56t95qlbw1w07l3340000gn/T/ipykernel_4424/3356709775.py:947: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Metal device set to: Apple M1 Max\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 10 Initial State [4 0 1 4 3 3 3 0 4 4] Steps: [8, 10, 11, 14, 16, 17, 17, 18, 26, 58]\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m ############################## BEST EPISODE LENGTH: 58 Executor: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:57:46,056 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592241152; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 64 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 20 Initial State [0 2 2 1 2 1 3 2 2 2] Steps: [8, 10, 11, 13, 13, 13, 13, 16, 16, 39]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 128 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 30 Initial State [0 3 2 3 1 0 2 3 4 2] Steps: [10, 10, 12, 12, 16, 17, 19, 19, 20, 28]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 192 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 40 Initial State [2 4 2 1 2 2 3 2 2 1] Steps: [11, 13, 13, 14, 16, 16, 18, 22, 22, 45]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:57:56,130 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592237056; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 256 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 50 Initial State [2 2 2 2 2 1 2 3 2 2] Steps: [10, 10, 13, 19, 22, 23, 24, 25, 26, 38]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 320 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 60 Initial State [2 2 1 2 3 2 2 2 2 4] Steps: [8, 10, 10, 10, 13, 13, 15, 17, 25, 28]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 384 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 70 Initial State [2 2 1 4 2 2 2 4 2 2] Steps: [8, 12, 12, 12, 14, 14, 18, 23, 25, 26]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:06,213 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592237056; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 448 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 80 Initial State [3 2 2 2 2 4 1 2 2 2] Steps: [8, 8, 9, 9, 10, 11, 15, 16, 18, 27]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 512 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 90 Initial State [2 2 1 2 2 2 2 2 2 1] Steps: [11, 15, 15, 16, 18, 21, 21, 25, 25, 26]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 576 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 100 Initial State [2 2 2 1 2 2 2 2 2 2] Steps: [9, 10, 11, 12, 12, 13, 18, 21, 27, 27]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 640 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 110 Initial State [0 4 0 2 4 3 1 2 2 2] Steps: [8, 8, 8, 9, 10, 10, 10, 10, 11, 11]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:16,283 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592089600; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 704 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 120 Initial State [0 2 2 2 2 2 2 1 2 3] Steps: [9, 9, 10, 10, 10, 10, 10, 10, 10, 10]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 768 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 130 Initial State [2 2 2 2 0 2 3 3 2 2] Steps: [8, 9, 9, 9, 10, 10, 10, 10, 10, 11]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 832 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 140 Initial State [2 1 0 2 3 2 2 2 2 2] Steps: [8, 8, 8, 8, 9, 9, 9, 10, 10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:26,356 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592089600; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 896 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 150 Initial State [2 3 1 2 2 2 2 2 2 1] Steps: [8, 8, 9, 9, 9, 9, 10, 10, 10, 10]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 960 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 160 Initial State [2 2 3 0 2 2 2 3 2 4] Steps: [8, 9, 9, 9, 9, 9, 9, 10, 10, 10]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1024 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 170 Initial State [4 2 2 2 4 0 2 2 2 1] Steps: [8, 8, 9, 9, 9, 9, 9, 10, 10, 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:36,432 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592089600; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1088 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 180 Initial State [0 2 2 2 4 2 2 2 2 2] Steps: [23, 24, 26, 28, 29, 32, 36, 37, 39, 41]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1152 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 190 Initial State [4 0 0 2 2 2 2 2 2 2] Steps: [103, 107, 108, 110, 110, 114, 142, 175, 177, 238]\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m ############################## BEST EPISODE LENGTH: 238 Executor: 0\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1216 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 200 Initial State [2 4 0 2 2 0 2 2 4 1] Steps: [35, 37, 39, 40, 42, 48, 51, 52, 76, 103]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:46,503 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592028160; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1280 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 210 Initial State [2 2 3 2 2 2 2 0 2 0] Steps: [28, 30, 34, 39, 46, 48, 48, 72, 74, 83]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1344 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 220 Initial State [3 2 2 2 2 2 2 2 2 2] Steps: [48, 55, 59, 72, 72, 76, 94, 102, 105, 146]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1408 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 230 Initial State [1 1 3 2 1 0 2 1 2 4] Steps: [36, 40, 40, 42, 44, 45, 46, 58, 64, 67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:58:56,582 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592032256; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1472 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 240 Initial State [2 2 3 0 2 2 1 2 2 4] Steps: [49, 54, 58, 58, 63, 64, 64, 66, 73, 82]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1536 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 250 Initial State [3 3 0 2 2 2 2 2 2 2] Steps: [76, 78, 85, 96, 105, 106, 107, 134, 135, 165]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1600 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 260 Initial State [2 1 2 2 4 4 2 2 2 2] Steps: [111, 112, 116, 135, 151, 152, 206, 235, 243, 297]\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m ############################## BEST EPISODE LENGTH: 297 Executor: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:06,661 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59592032256; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1664 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 270 Initial State [2 2 2 4 2 0 2 2 2 1] Steps: [156, 170, 170, 178, 182, 191, 223, 235, 275, 286]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1728 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 280 Initial State [2 2 2 0 2 2 2 2 2 1] Steps: [162, 164, 176, 176, 194, 195, 196, 203, 213, 241]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:16,745 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59591766016; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1792 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 290 Initial State [1 2 2 2 2 2 2 2 2 2] Steps: [170, 181, 248, 254, 396, 459, 467, 485, 584, 852]\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m ############################## BEST EPISODE LENGTH: 852 Executor: 0\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1856 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 300 Initial State [2 3 2 2 4 0 2 2 2 2] Steps: [9, 9, 9, 9, 9, 10, 10, 10, 10, 11]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1920 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:26,820 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59591766016; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 310 Initial State [2 2 2 2 2 2 2 2 2 2] Steps: [30, 33, 34, 35, 38, 41, 46, 52, 63, 118]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 1984 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 320 Initial State [2 2 2 1 2 2 2 2 0 2] Steps: [32, 33, 34, 34, 37, 39, 40, 65, 68, 70]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2048 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 330 Initial State [3 2 2 2 0 2 2 2 2 0] Steps: [29, 30, 31, 33, 36, 36, 38, 38, 39, 40]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2112 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:36,897 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59591811072; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 340 Initial State [0 2 2 0 2 2 2 4 1 2] Steps: [46, 52, 54, 63, 68, 70, 70, 74, 86, 92]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2176 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 350 Initial State [4 4 2 2 4 3 2 0 2 2] Steps: [50, 51, 52, 52, 59, 62, 67, 67, 72, 263]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2240 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 360 Initial State [1 2 1 2 2 3 2 2 2 2] Steps: [74, 79, 82, 88, 90, 98, 98, 101, 109, 114]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:46,970 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589160960; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2304 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 370 Initial State [4 2 2 2 2 2 4 1 2 2] Steps: [67, 95, 99, 101, 102, 108, 113, 124, 147, 179]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2368 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 12:59:57,051 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589160960; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 380 Initial State [2 2 2 1 3 2 4 2 2 4] Steps: [85, 87, 95, 113, 113, 120, 121, 153, 399, 2155]\n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m ############################## BEST EPISODE LENGTH: 2155 Executor: 0\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2432 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 390 Initial State [4 2 2 4 2 2 4 2 2 2] Steps: [84, 98, 103, 117, 136, 194, 275, 392, 659, 1051]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 13:00:07,129 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589427200; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2496 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 400 Initial State [2 2 2 2 2 2 2 2 2 2] Steps: [52, 56, 68, 72, 74, 94, 135, 145, 435, 845]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2560 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 410 Initial State [2 2 2 2 2 2 4 2 4 0] Steps: [38, 39, 59, 63, 69, 130, 185, 207, 269, 274]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 13:00:17,211 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589382144; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2624 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 420 Initial State [3 0 1 2 2 2 2 2 2 2] Steps: [35, 41, 45, 45, 50, 51, 79, 156, 190, 264]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2688 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 430 Initial State [0 2 2 2 4 3 2 1 0 2] Steps: [35, 40, 45, 59, 59, 60, 72, 185, 211, 291]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2752 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 440 Initial State [2 2 2 2 0 2 2 2 2 2] Steps: [35, 47, 54, 54, 54, 65, 68, 68, 78, 79]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 13:00:27,290 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589349376; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2816 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 450 Initial State [2 2 2 2 0 2 0 2 2 2] Steps: [34, 37, 41, 48, 49, 56, 60, 101, 169, 413]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2880 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 460 Initial State [2 2 2 1 2 2 2 1 2 2] Steps: [34, 39, 41, 42, 47, 49, 55, 59, 61, 170]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 13:00:37,372 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59589353472; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 2944 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 470 Initial State [2 0 2 2 2 0 2 2 2 2] Steps: [37, 39, 43, 46, 47, 75, 182, 295, 368, 406]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 3008 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 480 Initial State [1 0 0 2 2 2 2 2 2 2] Steps: [33, 36, 40, 51, 52, 59, 65, 69, 71, 277]\n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 3072 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 490 Initial State [2 0 2 4 2 2 2 2 2 2] Steps: [32, 32, 39, 44, 45, 47, 49, 49, 52, 54]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[33m(raylet)\u001b[0m [2023-02-26 13:00:47,456 E 4467 286253] (raylet) file_system_monitor.cc:105: /tmp/ray/session_2023-02-26_12-57-34_226349_4424 is over 95% full, available space: 59588194304; capacity: 1995218165760. Object creation will fail if spilling is required.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m LEARINING ITERATION: 3136 \n",
      "\u001b[2m\u001b[36m(Learner pid=4484)\u001b[0m \n",
      "\u001b[2m\u001b[36m(Executor pid=4486)\u001b[0m Ending Executor: 0 Episode 500 Initial State [2 2 1 1 4 2 2 4 2 0] Steps: [36, 43, 51, 53, 53, 55, 69, 80, 173, 525]\n",
      "RESULTS ['DONE! 0'] 196.87191081047058\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "\n",
    "from warnings import simplefilter \n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import ray, gym, time, math\n",
    "ray.RAY_memory_monitor_refresh_ms = 0 # Should stop providing memory warinings!!! but it does not !!!! \n",
    "\n",
    "from ray.util.queue import Queue\n",
    "from ray._private.utils import get_num_cpus\n",
    "\n",
    "cores = get_num_cpus()\n",
    "collect_obs = Queue()\n",
    "collect_examples = Queue()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa \n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.20f}\".format(x)})\n",
    "np.set_printoptions(precision=4, floatmode=\"maxprec\",suppress=True, linewidth=200)\n",
    "\n",
    "args = {# SYSTEM Settings\n",
    "        \"executors_n\": 1, #get_num_cpus(), # How many Executors are running in parallel\n",
    "\n",
    "        # GYM Settings\n",
    "        \"max_episodes\": 500, # 500 How many Episodes do you want to run?\n",
    "        \"num_envs_per_worker\": 10, # how many environments to run per worker -- tested till 512!!!\n",
    "        \"env_name\": 'CartPole-v1', # the name of the GYM Environment to run\n",
    "\n",
    "        \n",
    "        # MODEL Definition\n",
    "        \"state_n\": 4, # value of env.observation_space() (*2 as current_state and next_state)\n",
    "        \"state_n_adj\": 1, # all Task Specific Adjustments (*2 as current_state and next_state)\n",
    "        \"state_n_add\": 4, # all additional step descriptions (Reward, Done, Action, Discounted Reward)\n",
    "        \"action_n\": 2, # value of env.action_space()\n",
    "        \"common_layers_n\": [128,256,128], # Number of Neurons in Common Layers of A3C Model\n",
    "        \"value_layers_n\": [64,128,64], # Number of Neurons in Value Layers of A3C Model\n",
    "        \"policy_layers_n\": [64,128,64], # Number of Neurons in Policy Layers of A3C Model\n",
    "         \n",
    "        # LEARNER Settings\n",
    "        \"batch_size\": 128, # Number of examples per model update\n",
    "        \"model_alignment_frequency\": 128, # frequency of synchronization of the Target Model with On-Line Model -- OPTIONAL\n",
    "        \"minimum_model_update_frequency\": 64, # How Many Model Updates to run in each iteration at minimum.\n",
    "        \"epsilon_decay_policy\":1, # Run EDP? 0 No, 1 Yes\n",
    "        \"epsilon\": 0.1, # Original 0.1                 # e-greedy when exploring\n",
    "        \"epsilon_decay\": 0.995, # Original 0.995         # epsilon decay r\n",
    "        \n",
    "        # LEARNING RATE Decceleration\n",
    "        \"lr_alpha\": 0.0001, # Inital LR\n",
    "        \"lr_alpha_power\": 0.998, #0.998 bast so far # Controls the pace of LR depreciation\n",
    "        \"lr_alpha_limit\": 0.000001, # Lower limit on the LR\n",
    "        \n",
    "        # EXECUTOR Settings\n",
    "        \"internal_step_counter_limit\": 50000, # 500000 limit of steps per episode \n",
    "        \"experience_batch_size\": 1024, # how many steps to save into the memory buffer from each run\n",
    "        \"experience_max_batch_size\": 256, # Maximum number of cases to save to Memory Baffer from each experience (most recent) (tested 1024-64: 256 Best)\n",
    "        \n",
    "        # advarse STATE Probability\n",
    "        \"prob_advarse_state_initial\": 0.2, # Probability of choosing advarse case when starting new episode\n",
    "        \"prob_advarse_state_type_multiplier\": 0.0, #Adjustment to the Probability to control the ratio of issue types that are promoted through advarse initialization\n",
    "        \n",
    "        # REWARD Incentives\n",
    "        \"reward_negative\": -10.0, #-10.0, # Override the GYM's default negative return\n",
    "        \n",
    "        # PRIORITY MEMORY BUFFER\n",
    "        \"pmb_cols\": 5,  #\n",
    "        \"pmb_alpha\": 0.9, # Not Implemented                          # priority parameter, alpha=[0, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "        \"pmb_beta\": 0.4, # Tested till 0.8 but best reuslts with 1.0 # importance sampling parameter, beta=[0, 0.4, 0.5, 0.6, 1]\n",
    "        \"pmb_beta_increment\": 0.001, # Originally 0.001\n",
    "        \"pmb_td_error_margin\": 0.01,                                # pi = |td_error| + margin\n",
    "        \"pmb_abs_td_error_upper\": 1,\n",
    "               \n",
    "        }\n",
    "\n",
    "@ray.remote\n",
    "class Learner:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Set Parameters\n",
    "        self.executors_n = args[\"executors_n\"]\n",
    "        self.max_episodes = args[\"max_episodes\"]\n",
    "        self.env_name = args[\"env_name\"]\n",
    "        self.state_n = args[\"state_n\"]\n",
    "        self.action_n = args[\"action_n\"]\n",
    "        self.common_layers_n = args[\"common_layers_n\"]\n",
    "        self.value_layers_n = args[\"value_layers_n\"]\n",
    "        self.policy_layers_n = args[\"policy_layers_n\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.lr_alpha = args[\"lr_alpha\"]\n",
    "        self.lr_alpha_power = args[\"lr_alpha_power\"]\n",
    "        self.lr_alpha_limit = args[\"lr_alpha_limit\"]\n",
    "        self.prob_advarse_state_initial = args[\"prob_advarse_state_initial\"]\n",
    "        self.prob_advarse_state_type_multiplier = args[\"prob_advarse_state_type_multiplier\"]\n",
    "        self.internal_step_counter_limit = args[\"internal_step_counter_limit\"]\n",
    "        self.experience_batch_size = args[\"experience_batch_size\"]\n",
    "        self.reward_negative = args[\"reward_negative\"]\n",
    "        self.model_alignment_frequency = args[\"model_alignment_frequency\"]\n",
    "        self.experience_max_batch_size = args[\"experience_max_batch_size\"]\n",
    "        self.state_n_adj = args[\"state_n_adj\"]\n",
    "        self.state_n_add = args[\"state_n_add\"]\n",
    "        self.pmb_cols = args[\"pmb_cols\"]\n",
    "        self.pmb_alpha= args[\"pmb_alpha\"]\n",
    "        self.pmb_beta = args[\"pmb_beta\"]\n",
    "        self.pmb_beta_increment = args[\"pmb_beta_increment\"]\n",
    "        self.pmb_td_error_margin = args[\"pmb_td_error_margin\"]\n",
    "        self.pmb_abs_td_error_upper = args[\"pmb_abs_td_error_upper\"]\n",
    "        self.minimum_model_update_frequency = args[\"minimum_model_update_frequency\"]\n",
    "        self.epsilon_decay_policy = args[\"epsilon_decay_policy\"]\n",
    "        self.epsilon = args[\"epsilon\"]\n",
    "        self.epsilon_decay = args[\"epsilon_decay\"]\n",
    "        \n",
    "        self.collect_examples = []     \n",
    "        \n",
    "        \n",
    "        # Define BASE Model - Target\n",
    "        self.inputs_base = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "\n",
    "        self.common_network_base = Dense(self.common_layers_n[0], activation='relu',name=\"1\")(self.inputs_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[1], activation='relu',name=\"2\")(self.common_network_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[2], activation='relu',name=\"3\")(self.common_network_base)\n",
    "\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[0], activation='relu',name=\"7\")(self.common_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[1], activation='relu',name=\"8\")(self.policy_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[2], activation='relu',name=\"9\")(self.policy_network_base)\n",
    "\n",
    "        self.value_network_base = Dense(self.value_layers_n[0], activation='relu',name=\"4\")(self.common_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[1], activation='relu',name=\"5\")(self.value_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[2], activation='relu',name=\"6\")(self.value_network_base)\n",
    "\n",
    "        self.values_base = Dense(1,name=\"10\")(self.value_network_base)            \n",
    "        self.logits_base = Dense(self.action_n,name=\"11\")(self.policy_network_base)\n",
    "\n",
    "        self.model_base = Model(inputs=self.inputs_base, outputs=[self.values_base, self.logits_base])\n",
    "\n",
    "\n",
    "        # Define MAIN Model - Trainable Model\n",
    "        self.inputs_main = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "        self.common_network_main = Dense(self.common_layers_n[0], activation='relu')(self.inputs_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[1], activation='relu')(self.common_network_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[2], activation='relu')(self.common_network_main)\n",
    "\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[1], activation='relu')(self.policy_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[2], activation='relu')(self.policy_network_main)\n",
    "\n",
    "        self.value_network_main = Dense(self.value_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[1], activation='relu')(self.value_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[2], activation='relu')(self.value_network_main)\n",
    "\n",
    "        self.values_main = Dense(1)(self.value_network_main)\n",
    "        self.logits_main = Dense(self.action_n)(self.policy_network_main)\n",
    "        \n",
    "        self.model_main = Model(inputs=self.inputs_main, outputs=[self.values_main, self.logits_main])\n",
    "\n",
    "        # Define Optimizer\n",
    "        self.optimizer = tfa.optimizers.RectifiedAdam(self.lr_alpha)\n",
    "        \n",
    "        self.executor_model = self.model_main.get_weights()\n",
    "\n",
    "        #if counter_learninig % model_alignment_frequency == 0:\n",
    "        self.model_base.set_weights(self.model_main.get_weights())    \n",
    "\n",
    "        #executor_model.append(model_main.get_weights()) # the first call MUST be append to create the entry [0]\n",
    "        #print(\"Saved Model\", worker, len(executor_model))\n",
    "\n",
    "        self.memory_buffer = np.full((self.state_n+self.state_n_adj) * 2 + self.state_n_add + self.pmb_cols,0.0)\n",
    "        self.memory_buffer = []        \n",
    "\n",
    "        # GLOBAL COUNTERS -- possibly tobe moved to a separate worker\n",
    "        self.counter_learninig = 0\n",
    "        self.episode_counter = 0\n",
    "        self.executor_counter = 0        \n",
    "        self.steps_counter = 0\n",
    "        self.internal_step_counter_best = 0\n",
    "        \n",
    "    def get_executor_model(self):\n",
    "        return self.executor_model\n",
    "\n",
    "    def get_base_model_weights(self):\n",
    "        return self.model_base.get_weights()\n",
    "\n",
    "    def get_main_model_weights(self):\n",
    "        return self.model_main.get_weights()\n",
    "    \n",
    "    def increase_counter_learninig(self):\n",
    "        self.counter_learninig += 1\n",
    "\n",
    "    def increase_episode_counter(self):\n",
    "        self.episode_counter += 1\n",
    "\n",
    "    def increase_executor_counter(self):\n",
    "        self.executor_counter += 1\n",
    "        \n",
    "    def increase_steps_counter(self, value):\n",
    "        self.steps_counter += value\n",
    "\n",
    "    def set_internal_step_counter_best(self, value):\n",
    "        self.internal_step_counter_best = value\n",
    "    \n",
    "    def reset_counter_learninig(self):\n",
    "        self.counter_learninig = 0\n",
    "\n",
    "    def reset_episode_counter(self):\n",
    "        self.episode_counter = 0\n",
    "        \n",
    "    def reset_executor_counter(self):\n",
    "        self.executor_counter = 0        \n",
    "        \n",
    "    def reset_steps_counter(self):\n",
    "        self.steps_counter = 0\n",
    "        \n",
    "    def get_counter_learninig(self):\n",
    "        return self.counter_learninig\n",
    "\n",
    "    def get_episode_counter(self):\n",
    "        return self.episode_counter\n",
    "        \n",
    "    def get_executor_counter(self):\n",
    "        return self.executor_counter\n",
    "    \n",
    "    def get_steps_counter(self):\n",
    "        return self.steps_counter\n",
    "\n",
    "    def get_internal_step_counter_best(self):\n",
    "        return self.internal_step_counter_best\n",
    "\n",
    "    def train(self):\n",
    "        #print(\"LEARNER.train\")\n",
    "        #print(\"LEARNER RUN\", self.episode_counter)\n",
    "\n",
    "        # Adjust Monotonically Decreasing Learning Rate\n",
    "        self.next_lr_alpha = self.lr_alpha * np.power(self.lr_alpha_power, self.episode_counter)\n",
    "        if self.next_lr_alpha < self.lr_alpha_limit:\n",
    "            self.next_lr_alpha = self.lr_alpha_limit\n",
    "\n",
    "        self.optimizer.learning_rate = self.next_lr_alpha\n",
    "        \n",
    "        #print(\"next_lr_alpha\", self.next_lr_alpha)\n",
    "\n",
    "        # Initialize LEARNER\n",
    "        #while self.collect_examples.qsize() > 0: # and episod_counter.value < max_episodes:\n",
    "\n",
    "        #print(\"LEARNER len(self.collect_examples)\", len(self.collect_examples))\n",
    "        for i in self.memory_buffer:\n",
    "\n",
    "            self.example = i\n",
    "            #print(\"LEARNER self.example\", self.example.shape)\n",
    "            \n",
    "            #print(\"WEIGHTS BASE\", self.model_base.get_weights()[0][0][0])\n",
    "            #print(\"WEIGHTS MAIN\", self.model_main.get_weights()[0][0][0])\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                #print(\"CS\", example[:,:5])\n",
    "                self.values, self.logits = self.model_base(tf.convert_to_tensor(self.example[:,:5], dtype=tf.float32))\n",
    "                #print(\"Disc Reward\", example[:,-(pmb_cols+1)])\n",
    "                self.advantage = tf.convert_to_tensor(np.expand_dims(self.example[:,-(self.pmb_cols+1)],axis=1), dtype=tf.float32) - self.values\n",
    "                self.value_loss = self.advantage ** 2 # this is a term to be minimized in trainig \n",
    "                self.policy = tf.nn.softmax(self.logits)\n",
    "                self.entropy = tf.reshape(tf.nn.softmax_cross_entropy_with_logits(labels=self.policy, logits=self.logits), [-1,1])\n",
    "                #print(\"Action\", example[:,-(pmb_cols+2)])\n",
    "                self.policy_loss = tf.reshape(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=list(self.example[:,-(self.pmb_cols+2)].astype(int)), logits=self.logits), [-1,1])            \n",
    "                self.policy_loss *= tf.stop_gradient(self.advantage) # advantage will be exluded from computation of the gradient; thsi allows to treat the values as constants\n",
    "                self.policy_loss -= 0.01 * self.entropy # entropy adjustment for better exploration \n",
    "                self.total_loss = tf.reduce_mean((0.5 * self.value_loss + self.policy_loss))\n",
    "\n",
    "\n",
    "            self.grads = tape.gradient(self.total_loss, self.model_base.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(self.grads, self.model_main.trainable_weights))\n",
    "            \n",
    "            self.counter_learninig += 1\n",
    "\n",
    "            self.model_base.set_weights(self.model_main.get_weights()) ### the THREADED IMPLEMENTATION IS SYNCHRONIZED AT EACH STEP!!!\n",
    "\n",
    "        self.executor_model = self.model_main.get_weights()\n",
    "        \n",
    "        #print(\"LEARNER T BASE\", self.model_base.get_weights()[0][0][:5])\n",
    "        #print(\"LEARNER T MAIN\", self.model_main.get_weights()[0][0][:5])\n",
    "        \n",
    "        #print(type(self.executor_model))\n",
    "        #print(\"LEARNER T EXECUTOR\", self.executor_model[0][0][:5])        \n",
    "\n",
    "        #if counter_learninig % model_alignment_frequency == 0:\n",
    "        self.model_base.set_weights(self.model_main.get_weights())\n",
    "\n",
    "        print(\"LEARINING ITERATION:\", self.counter_learninig,\"\\n\")\n",
    "        \n",
    "        #self.steps_counter = 0\n",
    "        self.executor_counter = 0\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def update_memory_buffer(self, experience):\n",
    "        \n",
    "        #self.memory_buffer.append(experience)\n",
    "        self.memory_buffer = experience\n",
    "        #print(\"LEARNER UPDATE MB\", len(self.memory_buffer))\n",
    "        \n",
    "@ray.remote\n",
    "class Memorizer:\n",
    "    def __init__(self, learner):\n",
    "        \n",
    "        # Set Parameters\n",
    "        self.executors_n = args[\"executors_n\"]\n",
    "        self.max_episodes = args[\"max_episodes\"]\n",
    "        self.env_name = args[\"env_name\"]\n",
    "        self.state_n = args[\"state_n\"]\n",
    "        self.action_n = args[\"action_n\"]\n",
    "        self.common_layers_n = args[\"common_layers_n\"]\n",
    "        self.value_layers_n = args[\"value_layers_n\"]\n",
    "        self.policy_layers_n = args[\"policy_layers_n\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.lr_alpha = args[\"lr_alpha\"]\n",
    "        self.lr_alpha_power = args[\"lr_alpha_power\"]\n",
    "        self.lr_alpha_limit = args[\"lr_alpha_limit\"]\n",
    "        self.prob_advarse_state_initial = args[\"prob_advarse_state_initial\"]\n",
    "        self.prob_advarse_state_type_multiplier = args[\"prob_advarse_state_type_multiplier\"]\n",
    "        self.internal_step_counter_limit = args[\"internal_step_counter_limit\"]\n",
    "        self.experience_batch_size = args[\"experience_batch_size\"]\n",
    "        self.reward_negative = args[\"reward_negative\"]\n",
    "        self.model_alignment_frequency = args[\"model_alignment_frequency\"]\n",
    "        self.experience_max_batch_size = args[\"experience_max_batch_size\"]\n",
    "        self.state_n_adj = args[\"state_n_adj\"]\n",
    "        self.state_n_add = args[\"state_n_add\"]\n",
    "        self.pmb_cols = args[\"pmb_cols\"]\n",
    "        self.pmb_alpha= args[\"pmb_alpha\"]\n",
    "        self.pmb_beta = args[\"pmb_beta\"]\n",
    "        self.pmb_beta_increment = args[\"pmb_beta_increment\"]\n",
    "        self.pmb_td_error_margin = args[\"pmb_td_error_margin\"]\n",
    "        self.pmb_abs_td_error_upper = args[\"pmb_abs_td_error_upper\"]\n",
    "        self.minimum_model_update_frequency = args[\"minimum_model_update_frequency\"]\n",
    "        self.epsilon_decay_policy = args[\"epsilon_decay_policy\"]\n",
    "        self.epsilon = args[\"epsilon\"]\n",
    "        self.epsilon_decay = args[\"epsilon_decay\"]\n",
    "        \n",
    "        self.memory_buffer = np.full((self.state_n + self.state_n_adj) * 2 + self.state_n_add + self.pmb_cols,0.0)\n",
    "        self.memory_buffer[-1] = 99 \n",
    "        self.collect_obs = []\n",
    "        self.worker = \"Memorizer\"\n",
    "        \n",
    "        # Define BASE Model - Target\n",
    "        self.inputs_base = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "\n",
    "        self.common_network_base = Dense(self.common_layers_n[0], activation='relu',name=\"1\")(self.inputs_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[1], activation='relu',name=\"2\")(self.common_network_base)\n",
    "        self.common_network_base = Dense(self.common_layers_n[2], activation='relu',name=\"3\")(self.common_network_base)\n",
    "\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[0], activation='relu',name=\"7\")(self.common_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[1], activation='relu',name=\"8\")(self.policy_network_base)\n",
    "        self.policy_network_base = Dense(self.policy_layers_n[2], activation='relu',name=\"9\")(self.policy_network_base)\n",
    "\n",
    "        self.value_network_base = Dense(self.value_layers_n[0], activation='relu',name=\"4\")(self.common_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[1], activation='relu',name=\"5\")(self.value_network_base)\n",
    "        self.value_network_base = Dense(self.value_layers_n[2], activation='relu',name=\"6\")(self.value_network_base)\n",
    "\n",
    "        self.values_base = Dense(1,name=\"10\")(self.value_network_base)            \n",
    "        self.logits_base = Dense(self.action_n,name=\"11\")(self.policy_network_base)\n",
    "\n",
    "        self.model_base = Model(inputs=self.inputs_base, outputs=[self.values_base, self.logits_base])   \n",
    "        self.base_model_weights = ray.get(learner.get_base_model_weights.remote())\n",
    "        self.model_base.set_weights(self.base_model_weights)\n",
    "        \n",
    "        # Define MAIN Model - Trainable Model\n",
    "        self.inputs_main = tf.keras.Input(shape=(self.state_n+self.state_n_adj,))\n",
    "        self.common_network_main = Dense(self.common_layers_n[0], activation='relu')(self.inputs_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[1], activation='relu')(self.common_network_main)\n",
    "        self.common_network_main = Dense(self.common_layers_n[2], activation='relu')(self.common_network_main)\n",
    "\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[1], activation='relu')(self.policy_network_main)\n",
    "        self.policy_network_main = Dense(self.policy_layers_n[2], activation='relu')(self.policy_network_main)\n",
    "\n",
    "        self.value_network_main = Dense(self.value_layers_n[0], activation='relu')(self.common_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[1], activation='relu')(self.value_network_main)\n",
    "        self.value_network_main = Dense(self.value_layers_n[2], activation='relu')(self.value_network_main)\n",
    "\n",
    "        self.values_main = Dense(1)(self.value_network_main)\n",
    "        self.logits_main = Dense(self.action_n)(self.policy_network_main)\n",
    "        \n",
    "        self.model_main = Model(inputs=self.inputs_main, outputs=[self.values_main, self.logits_main])\n",
    "        self.main_model_weights = ray.get(learner.get_main_model_weights.remote())\n",
    "        self.model_main.set_weights(self.main_model_weights) \n",
    "        \n",
    "        #self.experience_length_all = np.array([]).reshape(0, self.executors_n)\n",
    "        self.experience_length_all = []\n",
    "        self.experience_length = []\n",
    "    \n",
    "    def get_experience_length_all(self):\n",
    "        return self.experience_length_all[1:]\n",
    "    \n",
    "    def get_collected_obs(self):\n",
    "        return self.collect_obs\n",
    "        \n",
    "    def collect(self, learner, experience):\n",
    "        #print(\"MEMORIZER.collect\")\n",
    "        \n",
    "        #print(\"MEMORIZER.collect - len(experience)\", len(experience), \"experience\")\n",
    "        self.collect_obs.append(experience)\n",
    "        \n",
    "        #print(\"MEMORIZER - EXECUTORS\", ray.get(learner.get_executor_counter.remote()))\n",
    "        \n",
    "        if ray.get(learner.get_executor_counter.remote()) < self.executors_n:\n",
    "            #print(\"WAITING IN MEMORIZER 1\", ray.get(learner.get_executor_counter.remote()), ray.get(learner.get_steps_counter.remote()), ray.get(learner.get_episode_counter.remote()))\n",
    "            pass\n",
    "        else:\n",
    "            #print(\"WAITING IN MEMORIZER 2\")\n",
    "            #print(\"MEMORY UPDATE\", self.collect_obs)\n",
    "            self.memory_update(learner, self.collect_obs)\n",
    "            #ray.get(learner.update_memory_buffer.remote(self.memory_buffer))\n",
    "            \n",
    "    def collect_length(self, experience_length):\n",
    "        self.experience_length.append(experience_length)\n",
    "        #print(\"MEMORIZER.collect_length\")\n",
    "            \n",
    "    #def memory_update(self, learner, collect_obs, collect_examples):\n",
    "    def memory_update(self, learner, collect_obs):\n",
    "        #print(\"MEMORIZER.memory_update\")\n",
    "        \n",
    "        while ray.get(learner.get_executor_counter.remote()) < self.executors_n:\n",
    "            pass\n",
    "        \n",
    "        self.base_model_weights = ray.get(learner.get_base_model_weights.remote())\n",
    "        self.model_base.set_weights(self.base_model_weights)\n",
    "        \n",
    "        self.main_model_weights = ray.get(learner.get_main_model_weights.remote())\n",
    "        self.model_main.set_weights(self.main_model_weights)\n",
    "        \n",
    "        #print(\"MEMORIZER MU BASE\", self.model_base.get_weights()[0][0][:5])\n",
    "        #print(\"MEMORIZER MU MAIN\", self.model_main.get_weights()[0][0][:5])\n",
    "        \n",
    "        self.mem_counter = 0\n",
    "        \n",
    "        #self.experience_length = np.zeros((1,self.executors_n))\n",
    "        #self.experience_length = []\n",
    "        \n",
    "        #print(\"BEFORE\", len(self.experience_length))\n",
    "        \n",
    "        #print(\"collect_obs\", len(collect_obs[0]), \"collect_obs\")\n",
    "        #print(\"BEFORE MEMORY BUFFER\", self.memory_buffer.shape, self.memory_buffer)\n",
    "        for i in collect_obs[0]:\n",
    "            self.mem_counter += 1\n",
    "            self.exp_temp = i\n",
    "            \n",
    "            #print(\"self.exp_temp\", self.exp_temp.shape, \"self.exp_temp\", \"self.exp_temp\")\n",
    "            \n",
    "            #self.experience_length[0,self.mem_counter] = self.exp_temp.shape[0]\n",
    "            #self.experience_length.append(self.exp_temp.shape[0])\n",
    "\n",
    "            #print(self.worker, \"self.exp_temp\", type(self.exp_temp), self.exp_temp)\n",
    "\n",
    "            #print(self.worker, mem_counter, \"memory_buffer B_STACK\", self.memory_buffer.shape)\n",
    "            #print(self.worker, mem_counter, \"exp_temp B_STACK\", self.exp_temp.shape)\n",
    "\n",
    "            self.memory_buffer = np.vstack((self.memory_buffer, self.exp_temp))\n",
    "            #print(self.worker, self.mem_counter, \"memory_buffer A_STACK\", self.memory_buffer.shape)\n",
    "            #print(self.worker, mem_counter, \"MIN INPUTS\", self.memory_buffer.shape[0], self.experience_batch_size * self.executors_n) \n",
    "            \n",
    "            self.memory_buffer = self.memory_buffer[-np.minimum(self.memory_buffer.shape[0], self.experience_batch_size * self.executors_n):,:]\n",
    "\n",
    "            #print(self.worker, self.mem_counter, \"memory_buffer A_MIN\", self.memory_buffer.shape)\n",
    "        \n",
    "        #self.experience_length_all = np.vstack([self.experience_length_all, self.experience_length])\n",
    "        \n",
    "        if self.memory_buffer[0,-1] == 99:\n",
    "            self.memory_buffer = self.memory_buffer[1:,:]\n",
    "            \n",
    "        #print(\"AFTER MEMORY BUFFER\", self.memory_buffer.shape, self.memory_buffer)\n",
    "        #print(\"AFTER\", len(self.experience_length))\n",
    "        \n",
    "        self.experience_length_all.append(self.experience_length)\n",
    "        \n",
    "        # PRIORITY MEMORY BUFFER\n",
    "\n",
    "        # Inverse Discounted Reward Probability\n",
    "        self.dr_min = np.min(self.memory_buffer[:,-(self.pmb_cols+1)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols)] = self.memory_buffer[:,-(self.pmb_cols+1)] - self.dr_min\n",
    "        self.dr_max = np.max(self.memory_buffer[:,-(self.pmb_cols)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols)] = 1 - self.memory_buffer[:,-(self.pmb_cols)] / self.dr_max + 0.01\n",
    "        self.dr_sum = np.sum(self.memory_buffer[:,-(self.pmb_cols)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols)] = self.memory_buffer[:,-(self.pmb_cols)] / self.dr_sum\n",
    "\n",
    "        # Inverse \"Age\" Probability\n",
    "        self.memory_buffer[:,-(self.pmb_cols-1)] += 1\n",
    "        self.age_max = np.max(self.memory_buffer[:,-(self.pmb_cols-1)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols-2)] = self.age_max - self.memory_buffer[:,-(self.pmb_cols-1)] + 1.0\n",
    "        self.age_sum = np.sum(self.memory_buffer[:,-(self.pmb_cols-2)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols-2)] = self.memory_buffer[:,-(self.pmb_cols-2)] / self.age_sum\n",
    "\n",
    "        # Proportional TD Error Probability                \n",
    "        self.target_q, self.target_logits = self.model_base(tf.convert_to_tensor(self.memory_buffer[:,5:10], dtype=tf.float32))\n",
    "        self.td_target = np.expand_dims(self.memory_buffer[:,10],axis = -1) + 0.9 * self.target_q * np.expand_dims((1 - self.memory_buffer[:,11]),axis = -1)\n",
    "        self.predict_q, self.predict_logits = self.model_main(tf.convert_to_tensor(self.memory_buffer[:,:5], dtype=tf.float32))\n",
    "        self.abs_td_error = np.abs(self.td_target - self.predict_q) + self.pmb_td_error_margin\n",
    "        self.clipped_td_error = np.where(self.abs_td_error < self.pmb_abs_td_error_upper, self.abs_td_error, self.pmb_abs_td_error_upper)                \n",
    "        self.memory_buffer[:,-(self.pmb_cols-3)] = self.clipped_td_error[:,0]\n",
    "        self.td_error_sum = np.sum(self.memory_buffer[:,-(self.pmb_cols-3)])\n",
    "        self.memory_buffer[:,-(self.pmb_cols-3)] = self.memory_buffer[:,-(self.pmb_cols-3)] / self.td_error_sum\n",
    "\n",
    "        self.memory_buffer[:,-1] = np.average(self.memory_buffer[:,[-(self.pmb_cols-2),-(self.pmb_cols-3)]], axis = 1 ) # Best! 280-310\n",
    "        self.pmb_beta = min(1., self.pmb_beta + self.pmb_beta_increment * self.executors_n)\n",
    "        self.memory_buffer[:,-1] = np.power(self.memory_buffer[:,-1], self.pmb_beta)\n",
    "        self.total_error_sum = np.sum(self.memory_buffer[:,-1])\n",
    "        self.memory_buffer[:,-1] = self.memory_buffer[:,-1] / self.total_error_sum\n",
    "\n",
    "        self.prob_sum_check1 = np.sum(self.memory_buffer[:,-(self.pmb_cols)])\n",
    "        self.prob_sum_check2 = np.sum(self.memory_buffer[:,-(self.pmb_cols-2)])\n",
    "        self.prob_sum_check3 = np.sum(self.memory_buffer[:,-(self.pmb_cols-3)])\n",
    "        self.prob_sum_check = np.sum(self.memory_buffer[:,-1])\n",
    "\n",
    "        self.batch_size_min = np.minimum(self.batch_size,self.memory_buffer.shape[0])\n",
    "        self.runs = self.memory_buffer.shape[0] // np.minimum(self.memory_buffer.shape[0], self.batch_size_min) + 1\n",
    "        self.runs = np.maximum(self.minimum_model_update_frequency, self.runs)\n",
    "        \n",
    "        collect_samples = []\n",
    "        \n",
    "        for i in range(self.runs):\n",
    "\n",
    "            self.sample_index = np.random.choice(self.memory_buffer.shape[0],\n",
    "                                            np.minimum(self.memory_buffer.shape[0], self.batch_size_min),\n",
    "                                            p = self.memory_buffer[:,-1],\n",
    "                                            replace=False)\n",
    "            \n",
    "            self.sample = self.memory_buffer[self.sample_index, :]\n",
    "            \n",
    "            #print(\"self.sample\", self.sample.shape, \"self.sample\")\n",
    "\n",
    "            collect_samples.append(self.sample)\n",
    "        \n",
    "        #print(\"self.sample\", self.sample)\n",
    "        \n",
    "        self.collect_obs = []\n",
    "        self.experience_length = []\n",
    "        \n",
    "        #print(\"MEMORIZER len(collect_examples)\", len(collect_examples))\n",
    "        ray.get(learner.update_memory_buffer.remote(collect_samples))\n",
    "        #print(\"MEMORIZER Before TRAIN\")\n",
    "        ray.get(learner.train.remote())\n",
    "        #print(\"MEMORIZER After TRAIN\")\n",
    "    \n",
    "@ray.remote\n",
    "class Executor:\n",
    "    #def __init__(self, memorizer, learner):\n",
    "    def __init__(self, i, args):\n",
    "        \n",
    "        # Set Parameters\n",
    "        self.executors_n = args[\"executors_n\"]\n",
    "        self.max_episodes = args[\"max_episodes\"]\n",
    "        self.env_name = args[\"env_name\"]\n",
    "        self.state_n = args[\"state_n\"]\n",
    "        self.action_n = args[\"action_n\"]\n",
    "        self.common_layers_n = args[\"common_layers_n\"]\n",
    "        self.value_layers_n = args[\"value_layers_n\"]\n",
    "        self.policy_layers_n = args[\"policy_layers_n\"]\n",
    "        self.batch_size = args[\"batch_size\"]\n",
    "        self.lr_alpha = args[\"lr_alpha\"]\n",
    "        self.lr_alpha_power = args[\"lr_alpha_power\"]\n",
    "        self.lr_alpha_limit = args[\"lr_alpha_limit\"]\n",
    "        self.prob_advarse_state_initial = args[\"prob_advarse_state_initial\"]\n",
    "        self.prob_advarse_state_type_multiplier = args[\"prob_advarse_state_type_multiplier\"]\n",
    "        self.internal_step_counter_limit = args[\"internal_step_counter_limit\"]\n",
    "        self.experience_batch_size = args[\"experience_batch_size\"]/16\n",
    "        self.reward_negative = args[\"reward_negative\"]\n",
    "        self.model_alignment_frequency = args[\"model_alignment_frequency\"]\n",
    "        self.experience_max_batch_size = args[\"experience_max_batch_size\"]\n",
    "        self.state_n_adj = args[\"state_n_adj\"]\n",
    "        self.state_n_add = args[\"state_n_add\"]\n",
    "        self.pmb_cols = args[\"pmb_cols\"]\n",
    "        self.pmb_alpha= args[\"pmb_alpha\"]\n",
    "        self.pmb_beta = args[\"pmb_beta\"]\n",
    "        self.pmb_beta_increment = args[\"pmb_beta_increment\"]\n",
    "        self.pmb_td_error_margin = args[\"pmb_td_error_margin\"]\n",
    "        self.pmb_abs_td_error_upper = args[\"pmb_abs_td_error_upper\"]\n",
    "        self.minimum_model_update_frequency = args[\"minimum_model_update_frequency\"]\n",
    "        self.epsilon_decay_policy = args[\"epsilon_decay_policy\"]\n",
    "        self.epsilon = args[\"epsilon\"]\n",
    "        self.epsilon_decay = args[\"epsilon_decay\"]\n",
    "        self.num_envs_per_worker = args[\"num_envs_per_worker\"]\n",
    "        \n",
    "        self.collect_examples = collect_examples\n",
    "        \n",
    "        # Establish Environment\n",
    "        #self.env = gym.make(self.env_name).unwrapped #unwrapped to access the behind the scenes elements of the environment\n",
    "        #self.env = gym.vector.make(self.env_name, num_envs=3).unwrapped #unwrapped to access the behind the scenes elements of the environment\n",
    "        \n",
    "        self.env = gym.vector.SyncVectorEnv([lambda: gym.make(self.env_name).env for _ in range(self.num_envs_per_worker)])\n",
    "        #self.env = gym.vector.SyncVectorEnv([lambda: gym.make(self.env_name) for _ in range(self.num_envs_per_worker)])\n",
    "        #self.env = gym.vector.AsyncVectorEnv([lambda: gym.make(self.env_name).env for _ in range(self.num_envs_per_worker)])\n",
    "        \n",
    "        #print(\"self.env.state\", self.env.state)\n",
    "        #print(\"self.env.state\", self.env.observations)\n",
    "        \n",
    "        #self.env = gym.vector.SyncVectorEnv([\n",
    "        #    lambda: gym.make(self.env_name),\n",
    "        #    lambda: gym.make(self.env_name),            \n",
    "        #    lambda: gym.make(self.env_name)])\n",
    "        \n",
    "        # Define A3C Model for Executors\n",
    "        #self.inputs_executor = tf.keras.Input(shape=(self.num_envs_per_worker,self.state_n + self.state_n_adj,))\n",
    "        self.inputs_executor = tf.keras.Input(shape=(self.state_n + self.state_n_adj,))\n",
    "        \n",
    "        self.common_network_executor = Dense(self.common_layers_n[0], activation='relu')(self.inputs_executor)\n",
    "        self.common_network_executor = Dense(self.common_layers_n[1], activation='relu')(self.common_network_executor)\n",
    "        self.common_network_executor = Dense(self.common_layers_n[2], activation='relu')(self.common_network_executor)\n",
    " \n",
    "        self.policy_network_executor = Dense(self.policy_layers_n[0], activation='relu')(self.common_network_executor)\n",
    "        self.policy_network_executor = Dense(self.policy_layers_n[1], activation='relu')(self.policy_network_executor)\n",
    "        self.policy_network_executor = Dense(self.policy_layers_n[2], activation='relu')(self.policy_network_executor)\n",
    "        \n",
    "        self.value_network_executor = Dense(self.value_layers_n[0], activation='relu')(self.common_network_executor)\n",
    "        self.value_network_executor = Dense(self.value_layers_n[1], activation='relu')(self.value_network_executor)\n",
    "        self.value_network_executor = Dense(self.value_layers_n[2], activation='relu')(self.value_network_executor)\n",
    "        \n",
    "        self.logits_executor = Dense(self.action_n)(self.policy_network_executor)\n",
    "        self.values_executor = Dense(1)(self.value_network_executor)\n",
    "        \n",
    "        self.model_executor = Model(inputs=self.inputs_executor, outputs=[self.values_executor, self.logits_executor])\n",
    "        \n",
    "        self.observations = []\n",
    "        self.iter_counter = 0\n",
    "        \n",
    "        self.internal_step_counter_all = 0\n",
    "        self.done_indeed = 0\n",
    "        self.done_done_all = np.zeros(3)\n",
    "        \n",
    "        \n",
    "    def experience_generator(self, i, learner, memorizer):\n",
    "        #def experience_generator(self, i, learner):        \n",
    "        \n",
    "        #print(\"EG BEOFRE WHILE\", i)\n",
    "        \n",
    "        self.worker = i\n",
    "        self.time_start = time.time()\n",
    "        \n",
    "        reload_model_weights = 1\n",
    "        \n",
    "        #while (ray.get(learner.get_episode_counter.remote()) < self.max_episodes) and (ray.get(learner.get_internal_step_counter_best.remote()) < self.internal_step_counter_limit):\n",
    "        #print(\"BEFORE FIRST WHILE\", ray.get(learner.get_episode_counter.remote()), self.max_episodes, self.iter_counter, self.internal_step_counter_limit)\n",
    "        while (ray.get(learner.get_episode_counter.remote()) < self.max_episodes) and (self.iter_counter < self.internal_step_counter_limit):\n",
    "            #print(\"AFTER FIRST WHILE\", ray.get(learner.get_episode_counter.remote()), self.max_episodes, self.iter_counter, self.internal_step_counter_limit)\n",
    "            #print(\"EG AFTER FIRST WHILE\", i)\n",
    "            \n",
    "            self.iter_counter += 1\n",
    "            \n",
    "            #print(\"reload_model_weights\", reload_model_weights)\n",
    "            \n",
    "            if reload_model_weights == 1:\n",
    "                self.model_executor.set_weights(ray.get(learner.get_executor_model.remote()))\n",
    "                reload_model_weights = 0\n",
    "            \n",
    "            #print(\"EG\", self.model_executor.get_weights()[0][0][:5])\n",
    "                \n",
    "            # Collect Examples & Save them in the Central Observation Repository\n",
    "            self.current_state = self.env.reset()\n",
    "            \n",
    "            #print(i, \"STATE AFTER INITIAL RESET\", self.current_state)\n",
    "            \n",
    "            \n",
    "            #print(\"EPISODE CONTER\", ray.get(learner.get_episode_counter.remote()))\n",
    "            # ENSURE EXPLORATION OF advarse STATES\n",
    "            if ray.get(learner.get_episode_counter.remote()) <= 1:\n",
    "                self.prob_advarse_state = self.prob_advarse_state_initial\n",
    "            else:\n",
    "                self.prob_advarse_state = np.clip(self.prob_advarse_state_initial/math.log(ray.get(learner.get_episode_counter.remote()),5), 0.05, 0.2)\n",
    "            \n",
    "            self.prob_random_state = 1-self.prob_advarse_state*4\n",
    "            \n",
    "            # CartPole position_start:\n",
    "            # 0: Close to the Left Edge\n",
    "            # 1: Close to the Right Edge\n",
    "            # 2: Normal, random start (env.restart())\n",
    "            # 3: Leaning Heavilly to the Left\n",
    "            # 4: Leaning Heavilly to the Right\n",
    "            \n",
    "            #print(i, \"self.current_state BEFORE\", self.current_state)\n",
    "            \n",
    "            # Choose one of the 5 scenarios with probabilities defined in p=()\n",
    "            self.pos_start = np.random.choice(5,\n",
    "                                              size = self.num_envs_per_worker, \n",
    "                                              p = (self.prob_advarse_state + self.prob_advarse_state_type_multiplier * self.prob_advarse_state,\n",
    "                                              self.prob_advarse_state + self.prob_advarse_state_type_multiplier * self.prob_advarse_state,\n",
    "                                              self.prob_random_state,\n",
    "                                              self.prob_advarse_state - self.prob_advarse_state_type_multiplier * self.prob_advarse_state,\n",
    "                                              self.prob_advarse_state - self.prob_advarse_state_type_multiplier * self.prob_advarse_state))\n",
    "            \n",
    "            #print(\"self.pos_start\", self.pos_start)\n",
    "            \n",
    "            rows = np.where((self.pos_start == 0) | (self.pos_start == 5))\n",
    "            self.current_state[rows,0] = -1.5 # -2.4 MIN\n",
    "            rows = np.where(self.pos_start == 1)\n",
    "            self.current_state[rows,0] = 1.5 # 2.4 MAX\n",
    "            rows = np.where(self.pos_start == 3)\n",
    "            self.current_state[rows,2] = -0.150 #-0.0.20943951023931953 MIN\n",
    "            rows = np.where(self.pos_start == 4)            \n",
    "            self.current_state[rows,2] = 0.150 #0.0.20943951023931953 MAX            \n",
    "\n",
    "            #print(i, \"self.current_state\", self.current_state)\n",
    "            \n",
    "            #self.env.state = self.current_state\n",
    "            \n",
    "            #print(i, \"self.current_state UPDATED\", self.current_state)\n",
    "            \n",
    "            self.env_id = 0\n",
    "            #print(dir(self.env))\n",
    "            for self.env_indivdual in self.env.envs:\n",
    "                #print(self.env_indivdual.state)\n",
    "                self.env_indivdual.state = self.current_state[self.env_id, :]\n",
    "                #print(self.env_indivdual.state)\n",
    "                self.env_id += 1\n",
    "            #self.env.observations = self.current_state\n",
    "            #self.env.blablabla = self.current_state\n",
    "            \n",
    "            \n",
    "            #print(i, \"self.current_state AFTER\", self.current_state)\n",
    "            #print(i, \"self.env.state\", self.env.state)\n",
    "            #print(i, \"self.env.observatons AFTER\", self.env.observations)\n",
    "            #print(i, \"self.env.blablabla AFTER\", self.env.blablabla)            \n",
    "            \n",
    "            # Custom State Representation Adjustment to help agent learn to be closer to the center\n",
    "            self.current_state = np.append(self.current_state,(self.current_state[:,0] * self.current_state[:,0]).reshape((-1,1)), axis = 1) \n",
    "            \n",
    "            #print(i, \"STATE AFER APPEND\", self.current_state) \n",
    "            \n",
    "            self.observations = np.empty((1,(self.state_n + self.state_n_adj) * 2 + 3, self.num_envs_per_worker))\n",
    "            self.done = np.array([False for i in range(self.num_envs_per_worker)])\n",
    "            self.internal_step_counter = 0\n",
    "            self.collect_obs = []\n",
    "            self.collect_obs_length = []\n",
    "            self.done_indeed = 0\n",
    "            #print(i, \"self.observations INITIAL SHAPE\", self.observations.shape)\n",
    "            \n",
    "            #print(\"self.done BEFORE\", self.done)\n",
    "            #print(i, \"BEFORE SECOND WHILE\", self.done_indeed == 0, self.internal_step_counter <= self.internal_step_counter_limit)\n",
    "            #while (not(self.done.all() == True)) and (self.internal_step_counter <= self.internal_step_counter_limit):\n",
    "            self.done_ind = 0\n",
    "            self.done_ind_new = 0\n",
    "            self.done_done_all = 0\n",
    "            self.internal_step_counter_joint = []\n",
    "            \n",
    "            while (self.done_indeed == 0) and (self.internal_step_counter <= self.internal_step_counter_limit):                \n",
    "                #print(i, \"WITHIN SECOND WHILE\", self.done, self.done_indeed == 0, self.internal_step_counter <= self.internal_step_counter_limit)\n",
    "                #print(i, \"self.current_state 2\", self.current_state, self.current_state.shape)\n",
    "                #self.values, self.logits = self.model_executor(tf.convert_to_tensor(np.array(np.expand_dims(self.current_state,axis=0)), dtype=tf.float32))\n",
    "                self.values, self.logits = self.model_executor(tf.convert_to_tensor(np.array(self.current_state), dtype=tf.float32))\n",
    "                \n",
    "                # EPSILON-GREEDY with DECAY POLICY\n",
    "                \n",
    "                if self.epsilon_decay_policy == 1:\n",
    "                    #epsilon *= (2-epsilon_decay)\n",
    "                    self.epsilon *= self.epsilon_decay\n",
    "                    if self.epsilon >= np.random.rand(): # Random-Informed\n",
    "                        #print(\"self.logits\", self.logits)\n",
    "                        #self.action = np.argmax(self.logits, axis = 2).squeeze()\n",
    "                        self.action = np.argmax(self.logits, axis = 1).squeeze() #######\n",
    "                        #print(\"ACTION\", self.action)\n",
    "                        #action = np.random.choice(action_n)\n",
    "                        #stochastic_action_probabilities = tf.nn.softmax(logits)\n",
    "                        #action = np.random.choice(action_n, p=stochastic_action_probabilities.numpy()[0])\n",
    "                    else: # Greedy\n",
    "                        self.stochastic_action_probabilities = tf.nn.softmax(self.logits)\n",
    "                        #print(\"self.stochastic_action_probabilities.numpy()[0]\",self.stochastic_action_probabilities.numpy()[0])\n",
    "                        \n",
    "                        self.action = np.array([])\n",
    "                        #print(self.action, self.action_n, self.stochastic_action_probabilities.numpy())\n",
    "                        for i in range(self.num_envs_per_worker):\n",
    "                            #self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[0][i])))\n",
    "                            self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[i])))      #########                      \n",
    "                        #print(i, \"self.action 1\", self.action)\n",
    "                        #print(1, self.stochastic_action_probabilities, self.action)\n",
    "                        #action = np.argmax(logits) # Total Collapse\n",
    "                else:\n",
    "                    #self.stochastic_action_probabilities = tf.nn.softmax(self.logits)\n",
    "                    #self.action = np.random.choice(self.action_n, p=self.stochastic_action_probabilities.numpy()[0])\n",
    "                    \n",
    "                    self.stochastic_action_probabilities = tf.nn.softmax(self.logits)\n",
    "                    #print(\"self.stochastic_action_probabilities.numpy()[0]\",self.stochastic_action_probabilities.numpy()[0])\n",
    "\n",
    "                    self.action = np.array([])\n",
    "                    for i in range(self.num_envs_per_worker):\n",
    "                        #self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[0][i])))\n",
    "                        self.action = np.append(self.action, int(np.random.choice((self.action_n), p=self.stochastic_action_probabilities.numpy()[i]))) ########\n",
    "\n",
    "                    #print(2, self.stochastic_action_probabilities, self.action)                        \n",
    "                self.action = self.action.astype(int)\n",
    "            \n",
    "                #print(i, \"self.action 2\", self.action)\n",
    "                #for i in self.action:\n",
    "                #    print(type(i))\n",
    "                \n",
    "                #print(\"self.action\", self.action)\n",
    "                self.next_state, self.reward, self.done, self.info = self.env.step(self.action)\n",
    "\n",
    "                #print(\"NEXT STATE\", self.next_state)\n",
    "                #print(\"STEP 1\", self.next_state, self.reward, self.done, self.info)\n",
    "                \n",
    "                # DOES NOT WORK WITH VECTORIZED ENVIRONMENTS\n",
    "                #self.maybe_end_state = self.env.monitor.flush(force=True)\n",
    "                #print(\"maybe next state\", np.round(self.maybe_end_state,4), np.round(self.next_state,4))\n",
    "                \n",
    "                #print(\"current_state/next_state\", np.round(self.current_state[:,0],4), np.round(self.next_state[:,0],4))\n",
    "                \n",
    "                #print(\"AFTER STEP\", self.done)\n",
    "\n",
    "                #NEXT STEP (3, 4) (3,) (3,) 3\n",
    "                \n",
    "                #print(\"NEXT STEP\", type(self.next_state), type(self.reward), type(self.done), type(self.info))\n",
    "                #print(\"NEXT STEP\", self.next_state.shape, self.reward.shape, self.done.shape, len(self.info))\n",
    "                \n",
    "                #self.next_state = np.append(self.next_state, self.next_state[0] * self.next_state[0])\n",
    "                self.next_state = np.append(self.next_state,(self.next_state[:,0] * self.next_state[:,0]).reshape((-1,1)), axis = 1)\n",
    "                \n",
    "                #print(\"SQUARED\", (self.next_state[:,0] * self.next_state[:,0]).reshape((-1,1)))\n",
    "                #print(\"STEP 2\", self.next_state, self.reward, self.done, self.info)                \n",
    "\n",
    "                #print(\"self.next_state\", self.next_state)\n",
    "                # Add desired-behaviour incentive to the reward function\n",
    "                self.R_pos = 1*(1-np.abs(self.next_state[:, 0])/2.4) # 2.4 max value ### !!! in documentation it says 4.8 but failes beyound 2.4\n",
    "                self.R_ang = 1*(1-np.abs(self.next_state[:, 2])/0.20943951023931953) ### !!! in documentation it says 0.418 max value\n",
    "\n",
    "                #print(\"SHAPES BEFORE\", self.reward.shape, self.R_pos.shape, self.R_ang.shape)\n",
    "                #print(\"REWARD BEFORE\", self.reward, self.R_pos, self.R_ang)\n",
    "                self.reward = self.reward + self.R_pos + self.R_ang\n",
    "                \n",
    "                #print(\"self.reward AFTER\", self.reward)\n",
    "                \n",
    "                # Custom Fail Reward to speed up Learning of conseqences of being in advarse position\n",
    "                #print(\"self.done\", self.done)\n",
    "                \n",
    "                rows = np.where(self.done == True)\n",
    "                \n",
    "                #print(\"BEFORE self.reward NEGATIVE\", self.reward, rows)\n",
    "                self.reward[rows] = self.reward_negative # ST Original -1\n",
    "                #self.reward[rows-1] = self.reward_negative # ST Original -1\n",
    "                #print(\"AFTER self.reward NEGATIVE\", self.reward, rows)                \n",
    "                #if self.done == True: \n",
    "                #    self.reward = self.reward_negative # ST Original -1\n",
    "                        \n",
    "                #current_observation = np.append(current_state,(reward, done, action))\n",
    "                \n",
    "                #(Executor pid=61660) SHAPES BEFORE STACKING 0 (3, 5) (16,)\n",
    "                #(Executor pid=61660) SHAPES BEFORE STACKING 1 (31,) (3,) (3,) (3,)\n",
    "                #(Executor pid=61660) SHAPES BEFORE STACKING 2 (1, 13, 3) (40,)\n",
    "                \n",
    "                #print(\"SHAPES BEFORE STACKING 0\", self.current_state.shape, self.next_state.shape)\n",
    "                \n",
    "                #print(\"BEFORE STACKING\", self.current_state, self.next_state)\n",
    "                #self.current_observation = np.append(self.current_state, self.next_state)\n",
    "                self.current_observation = np.concatenate([self.current_state, self.next_state], axis = 1)\n",
    "                #print(\"AFTER STACKING\", self.current_observation)\n",
    "                \n",
    "                #print(\"SHAPES BEFORE STACKING 1\", self.current_observation.shape, self.reward.shape, self.done.shape, self.action.shape)\n",
    "                #print(\"BEFORE STACKING 1\", self.current_observation, self.reward, self.done, self.action)\n",
    "                \n",
    "                #self.current_observation = np.append(self.current_observation,(self.reward, self.done, self.action))\n",
    "                self.current_observation = np.concatenate([self.current_observation, self.reward.reshape([-1,1]), self.done.reshape([-1,1]), self.action.reshape([-1,1])], axis = 1)\n",
    "                \n",
    "                #print(\"AFTER STACKING 1\", self.current_observation)\n",
    "                \n",
    "                #print(\"BEFORE RESHAPE\", self.current_observation)\n",
    "                #print(\"AFTER RESHAPE\", self.current_observation.reshape([1,13,self.num_envs_per_worker]))\n",
    "                #print(\"EXPAND DIM\", np.expand_dims(np.swapaxes(self.current_observation,0,1), axis = 0))\n",
    "                \n",
    "                #print(\"BEFORE RESHAPE\", self.current_observation.shape)\n",
    "                #print(\"AFTER RESHAPE\", self.current_observation.reshape([1,13,self.num_envs_per_worker]).shape)\n",
    "                #print(\"EXPAND DIM\", np.expand_dims(np.swapaxes(self.current_observation,0,1), axis = 0).shape)                \n",
    "                \n",
    "                #print(\"SHAPES BEFORE STACKING 2\", self.observations.shape, self.current_observation.shape, self.current_observation[0])\n",
    "                #self.observations = np.vstack((self.observations, self.current_observation.reshape([1,13,self.num_envs_per_worker])))\n",
    "                \n",
    "                #print(\"BEFORE SWAPPING\", self.current_observation)\n",
    "                self.observations = np.vstack((self.observations, np.expand_dims(np.swapaxes(self.current_observation,0,1), axis = 0)))\n",
    "                #print(\"AFTER SWAPPING\", self.observations)\n",
    "                \n",
    "                if (self.observations.shape[0] > 1) and ((self.observations.shape[0] % self.experience_max_batch_size) == 0):\n",
    "                    self.observations = self.observations[-self.experience_max_batch_size:,:,:]\n",
    "                #print(\"SHAPES AFTER STACKING \", self.observations.shape, self.observations[:,:,0])\n",
    "\n",
    "                self.current_state = self.next_state\n",
    "                self.internal_step_counter += 1\n",
    "                  \n",
    "                if self.internal_step_counter == 1:\n",
    "                    self.observations = self.observations[1:,:,:]\n",
    "                    #print(\"self.observations\",self.observations.shape, self.observations)\n",
    "                \n",
    "                #print(\"self.done END\", self.done)\n",
    "                \n",
    "                if self.done.any() == True:\n",
    "                    self.done_ind = self.done * 1\n",
    "                    #print(\"self.done_ind\", self.done_ind, self.done_ind.sum(), self.done_done_all)\n",
    "                    self.done_ind_new = np.where((self.done_ind - self.done_done_all) == 1)\n",
    "                    #print(\"self.done_ind_new\", self.done_ind_new, self.done_ind_new[0].shape)\n",
    "                    \n",
    "                    if self.done_ind_new[0].shape[0] > 0:\n",
    "                        for exp in self.done_ind_new[0]:\n",
    "                            \n",
    "                            #print(\"exp\", exp)\n",
    "                            self.observations_current = self.observations[:,:,exp]\n",
    "                            self.terminal_state = self.info[exp][\"terminal_observation\"]\n",
    "                            #print(\"1 CURRENT STATE [-1]\", self.observations_current[-1], \"TERMINAL STATE\", self.terminal_state)\n",
    "                            self.terminal_state = np.append(self.terminal_state, self.terminal_state[0] * self.terminal_state[0])\n",
    "                            #print(\"self.terminal_state SHAPE\", self.terminal_state.shape)\n",
    "                            self.observations_current[-1,5:10] = self.terminal_state\n",
    "                            \n",
    "                            #self.observations_current = self.observations_current[1:,:]\n",
    "                            \n",
    "                            #print(\"CURRENT STATE\", self.observations_current, self.done)\n",
    "                            #print(\"self.info\", self.info)\n",
    "                            #print(\"CURRENT STATE [-1]\", self.observations_current[-1])\n",
    "                            #print(\"2 CURRENT STATE [-1]\", self.observations_current[-1], \"TERMINAL STATE\", self.terminal_state)\n",
    "\n",
    "                            self.collect_obs_length.append(self.observations_current.shape[0]) ### WRONG as now max length is self.experience_max_batch_size\n",
    "                            #print(\"self.collect_obs_length LEN()\", len(self.collect_obs_length), \"self.observations_current.shape[0]\", self.observations_current.shape[0])\n",
    "\n",
    "                            self.observations_current = self.observations_current[-np.minimum(self.observations_current.shape[0], self.experience_max_batch_size):]\n",
    "                            #print(\"self.observations_current\", self.observations_current)\n",
    "\n",
    "                            self.exp_len = self.observations_current.shape[0]\n",
    "                            #print(\"self.exp_len\", self.exp_len)\n",
    "                            self.exp_indices = np.array(range(self.exp_len)) + 1\n",
    "                            #print()\n",
    "                            self.rewards = np.flip(self.observations_current[:,(self.state_n + self.state_n_adj) * 2 ])\n",
    "                            \n",
    "                            # !!!!!!!! POTENTIAL NEGATIVE REWARD UPDATE !!!!!!!!\n",
    "                            #self.reward[1] = self.reward_negative\n",
    "                            \n",
    "                            #print(\"selfrewards\",self.rewards)\n",
    "                            self.discounted_rewards = np.empty(self.exp_len)\n",
    "                            #print(\"self.discounted_rewards\", self.discounted_rewards)\n",
    "                            self.reward_sum = 0\n",
    "\n",
    "                            if self.observations_current[-1,-2] == 0:\n",
    "                                # IN CASE THE EPISODE HAS NTO TERMINATED\n",
    "                                self.observations_current[-1,-2] = 2                        \n",
    "                                self.gamma = np.full(self.exp_len, 0.99)\n",
    "                                #print(1)\n",
    "                            else:\n",
    "                                # IN CASE THE EPISODE HAS TERMINATED\n",
    "                                #print(2)\n",
    "                                #print(\"exp_indices\", exp_indices)\n",
    "                                self.gamma = np.clip(0.0379 * np.log(self.exp_indices-1) + 0.7983, 0.5, 0.99)\n",
    "                            #print(\"END SHAPE\", self.observations.shape)    \n",
    "                            if self.observations_current[-1,-2] == 1:\n",
    "                                self.gamma[0] = 1\n",
    "                                #print(3)\n",
    "\n",
    "                            for step in range(self.exp_len):\n",
    "                                self.reward_sum = self.rewards[step] + self.gamma[step] * self.reward_sum\n",
    "                                self.discounted_rewards[step] = self.reward_sum    \n",
    "\n",
    "                            self.discounted_rewards = np.flip(self.discounted_rewards)\n",
    "                            \n",
    "                            #print(\"BEFORE self.observations_current\", np.round(np.float32(self.observations_current),4))\n",
    "                            #print(\"BEFORE self.discounted_rewards\", np.round(np.float32(self.discounted_rewards),4))\n",
    "\n",
    "                            self.observations_current = np.hstack((self.observations_current, np.expand_dims(self.discounted_rewards, axis = 1)))\n",
    "                            #print(\"AFTER self.observations_current\", np.round(np.float32(self.observations_current),4))                            \n",
    "                            self.observations_current = np.hstack((self.observations_current, np.zeros((self.observations_current.shape[0], self.pmb_cols))))\n",
    "\n",
    "                            #print(self.worker, ray.get(learner.get_episode_counter.remote()), \"observations\", self.observations_current.shape)\n",
    "                            \n",
    "                            #print(self.observations_current.shape, np.round(self.observations_current,4))\n",
    "                            \n",
    "                            # !!!!!!!! POTENTIAL NEGATIVE REWARD UPDATE !!!!!!!!\n",
    "                            #self.collect_obs.append(self.observations_current[:-1,:])\n",
    "                            \n",
    "                            \n",
    "                            self.collect_obs.append(self.observations_current)\n",
    "                            \n",
    "                            #print(\"EXECUTOR - self.collect_obs LEN\", len(self.collect_obs), \"self.observations_current.shape\", self.observations_current.shape)\n",
    "                            \n",
    "                            self.internal_step_counter_all += self.internal_step_counter\n",
    "                            \n",
    "\n",
    "                            #print(\"BEFORE\", self.done_done_all)\n",
    "                            self.done_done_all = np.maximum(self.done_done_all, self.done_ind)\n",
    "                            \n",
    "                            self.internal_step_counter_joint.append(self.internal_step_counter)\n",
    "\n",
    "                            #print(\"AFTER\", self.done_done_all, self.done_done_all.sum(), self.num_envs_per_worker)\n",
    "                            \n",
    "                            ray.get(learner.increase_episode_counter.remote())                                                     \n",
    "\n",
    "                        if self.done_done_all.sum() == self.num_envs_per_worker:\n",
    "\n",
    "                            #print(\"1\")\n",
    "                            self.done_indeed = 1\n",
    "                            ray.get(learner.increase_executor_counter.remote())                                \n",
    "\n",
    "                            #print(\"self.collect_obs\", self.collect_obs)\n",
    "\n",
    "                            #print(\"2\")\n",
    "\n",
    "                            # Update Counters to Track Progress\n",
    "\n",
    "\n",
    "                            #self.internal_step_counter_all += self.internal_step_counter\n",
    "\n",
    "\n",
    "                            #print(\"3\")\n",
    "                            if self.internal_step_counter_all < self.experience_batch_size:\n",
    "                                pass\n",
    "                            else:\n",
    "                                self.internal_step_counter_all = 0  \n",
    "                                ray.get(learner.increase_executor_counter.remote())\n",
    "\n",
    "                            print(\"Ending Executor:\", self.worker, \"Episode\", ray.get(learner.get_episode_counter.remote()), \"Initial State\", self.pos_start, \"Steps:\", self.internal_step_counter_joint)\n",
    "\n",
    "                            #print(\"4\")\n",
    "                            if self.internal_step_counter >= ray.get(learner.get_internal_step_counter_best.remote()):\n",
    "                                learner.set_internal_step_counter_best.remote(self.internal_step_counter)\n",
    "\n",
    "                                print(\"############################## BEST EPISODE LENGTH:\", self.internal_step_counter, \"Executor:\", self.worker)       \n",
    "\n",
    "                            #print(\"5\")\n",
    "                            if ray.get(learner.get_internal_step_counter_best.remote()) >= self.internal_step_counter_limit:\n",
    "\n",
    "                                self.episod_counter_target = ray.get(learner.get_episode_counter.remote())\n",
    "\n",
    "                                print(\"\\nREACHED GOAL of\", self.internal_step_counter_limit,  \"Steps in\", self.episod_counter_target, \"episodes; Learning Iterations (Not Available); in\",time.time()-self.time_start, \"seconds \\n\")  \n",
    "\n",
    "                            #ray.get(memorizer.collect.remote(learner, self.collect_obs))\n",
    "                            ray.get(memorizer.collect.remote(learner, self.collect_obs))                             \n",
    "                            ray.get(memorizer.collect_length.remote(self.collect_obs_length))\n",
    "                            \n",
    "                            self.collect_obs = []\n",
    "                            self.collect_obs_length = []                                \n",
    "\n",
    "                            #print(\"6\")\n",
    "                            while ray.get(learner.get_executor_counter.remote()) >= self.num_envs_per_worker:\n",
    "                                #print(\"WAITING FOR A NEW MODEL\")\n",
    "                                pass\n",
    "\n",
    "                            reload_model_weights = 1\n",
    "                        \n",
    "            #self.observations = np.empty((1,(self.state_n + self.state_n_adj) * 2 + 3, self.num_envs_per_worker))\n",
    "            #self.done_indeed = 0\n",
    "                \n",
    "\n",
    "@ray.remote\n",
    "def train(i, memorizer, learner, args):\n",
    "    #@ray.remote\n",
    "    #def train(i, learner):    \n",
    "    #executor = Executor.remote(memorizer, learner)\n",
    "    executor = Executor.remote(i, args)\n",
    "    ray.get(executor.experience_generator.remote(i, learner, memorizer))\n",
    "    #executor.experience_generator.remote(i, learner, memorizer)\n",
    "    \n",
    "    return \"DONE! \" + str(i)\n",
    "    \n",
    "learner = Learner.remote()\n",
    "memorizer = Memorizer.remote(learner)\n",
    "#executor = Executor.remote(memorizer, learner)    \n",
    "\n",
    "time_start = time.time()\n",
    "results = ray.get([train.remote(i, memorizer, learner, args) for i in range(int(args[\"executors_n\"]))])\n",
    "#results = ray.get([train.remote(i, learner) for i in range(int(cores))])\n",
    "print(\"RESULTS\", results, time.time()-time_start)\n",
    "    \n",
    "#print(1, ray.get(learner.get_episode_counter.remote()))\n",
    "#ray.get(learner.reset_episode_counter.remote())\n",
    "#print(2, ray.get(learner.get_episode_counter.remote()))\n",
    "\n",
    "# Do function from which to run it and initialize executors within each instance of the function.\n",
    "# within executor run memorizer collect function that will collect experience to the moemory buffer and check if enough experience has been collected and if so it will run the memorization proces - possibly memorization process can be coded in a separate fucntion\n",
    "\n",
    "\n",
    "print(ray.get(learner.get_executor_counter.remote()))\n",
    "\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e25051",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
