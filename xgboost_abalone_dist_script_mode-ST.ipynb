{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and deploy a regression model with the Amazon SageMaker XGBoost Algorithm using Script Mode\n",
    "_**Distributed training for regression with Amazon SageMaker XGBoost script mode**_\n",
    "\n",
    "---\n",
    "\n",
    "## Runtime\n",
    "\n",
    "This notebook takes approximately 5 minutes to run.\n",
    "\n",
    "## Contents\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "  1. [Fetch the dataset](#Fetch-the-dataset)\n",
    "  1. [Ingest data](#Ingest-data)\n",
    "1. [Create an XGBoost training script](#Create-an-XGBoost-training-script)\n",
    "1. [Train the XGBoost model](#Train-the-XGBoost-model)\n",
    "1. [Deploy the XGBoost model](#Deploy-the-XGBoost-model)\n",
    "1. [Cleanup](#Cleanup)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEBTAC's ANALYSIS SUMMARY\n",
    "\n",
    "OBJECTIVE:\n",
    "- Analyze the parallelization implemented for the XGBoost Algorithm in Amazon SageMaker. Two implementations tested:\n",
    "    - XGBoost as a framework\n",
    "    - XGBoost as a built-in algorithm\n",
    "\n",
    "BASED ON:\n",
    "- xgboost_abalone_dist_script_mode.ipynb from amazon-sagemaker-examples\n",
    "- https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\n",
    "\n",
    "MODIFICATIONs:\n",
    "- data_split() to prodcue 4 files each with up to 10 times more data than original file and additionaly copy-duplicated to produce final 32 files for GPU testing\n",
    "- adding TRAIN TIME INSIDE.PY for XGBoost as a framework to bring time.measuremnt as close to the training step as possible\n",
    "\n",
    "METHODOLOWGY:\n",
    "- confirmation of parallelization is provided by the execution time measures with expectation that the stronger node and the higher level of parallelization the shorter execution time\n",
    "\n",
    "- the desired time measure would caputere only time spent on training and exclude any time spent on instance initial data download and any output generation and instance closing - not all measures guarantee it but not all measures are available in all scenarios.\n",
    "\n",
    "- there are up to 3 time measures used inthe analyses\n",
    "    - TRAIN TIME OUTSIDE\n",
    "        - Execution time of the .fit() method - generally, it provides very little differentiation in the recorded time due to capturte of time required to start and stop the EC2 instances.\n",
    "    - TRAIN TIME INSIDE.PY OR \"@tracker xx.xxxxxxxxxxxxxx secs between node start and job finish\"\n",
    "        - those measures are intended to capture only the time behind excution of the XGBoost algorithm\n",
    "        - TRAIN TIME INSIDE.PY is available for the IN FRAMEWORK tests and it does exclude any initial data preparation and closing seps from measuremnt\n",
    "        - \"@tracker xx.xxxxxxxxxxxxxx secs between node start and job finish\" available for IN ALGORITHM tests but it seems to also cover soem data preparation and output generation steps in measurements thus those valus are not comperabel with the earlier measure\n",
    "            - this measre is not avaibale for single instance tests ?!?!?!??!\n",
    "    - Billable seconds: 592/4\n",
    "        - this is inernal AWS measure. it seems to capture well the effects of parallelization. On the surface, its values are contrary to expectaitons (i.e. increasing parallelization usually increases its value. This is so as it measures sum of Billable seconds accross all runnig instances. Thus the final measure is the average execution time per insance: Billable Secouds / Number of Instances. On this level, increaing the insance count does lead to the shorter execution time per instance. But this measure also captures time spent on data and output preparation.\n",
    "        \n",
    "- It would be best to implment the equivalent of TRAIN TIME INSIDE.PY measure for the built-in algorithm but this requires creation of a custom container which is not the objective of this work\n",
    "\n",
    "FINDINGS:\n",
    "- General\n",
    "    - Both XGBoost framework and built-in algorithm provide positive effects of parallelization\n",
    "        - in theis analysis, it is not possibel to say which approach is more effcient due to the mix of availabel measures andinclusion in some of them of external elements to the actual model traing.\n",
    "    - for clearest impact it is important to work with higher sample sizes thus increase in file sizes and their number.\n",
    "    - No \"Pipe\" Input-Data mode testing as it seems not to work with LIBSVM format.\n",
    "    - Data Sharding provides beneffits in multi instance training. the effect might be present also w???!?!?!?!?!?!?!?!*&^*&^*&%*%*&%&%%*%&&*%**%*^$*$#%#*%^#*\n",
    "    \n",
    "- XGBoost as a FRAMEWORK\n",
    "    - both update of the instance and increase of its count speeds up training.\n",
    "        - the effect is most visible between 1 and 2 insstances. it flattens when edding more instances but this is possibly due to the still low sample sizes  \n",
    "    - training pn GPUs has strong possitive effect possibly even in terms of cost-efficiency\n",
    "        - training on instance with 8 GPUs does not provide farhter speed-up. This can be due two factors:\n",
    "            - to small data\n",
    "            - framework implementation utilizes only single GPU per instance thus we can parallelize GPU execution only withmultiple instances -- requires further research\n",
    "            - data and output preparation takes significnat portion of the execution time with available sample size.\n",
    "\n",
    "- XGBoost as a built-in ALGORITH\n",
    "    - Same results as for the FRAMEWORK\n",
    "    - \"FastFile\" InputData mode - slower but possibly due to the small sample sizes\n",
    "    - Multi-GPU training indicates some speed-up but it is dificult to say if this is due to the stronger GPU type or due to parallization. the week effect can be attrubted to still too-small sample size.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMaker XGBoost to train and host a regression model. [XGBoost (eXtreme Gradient Boosting)](https://xgboost.readthedocs.io) is a popular and efficient machine learning (ML) algorithm used for regression and classification tasks on tabular datasets. It implements a technique know as gradient boosting on trees, and performs remarkably well in ML competitions, and gets a lot of attention from customers. \n",
    "\n",
    "We use the [Abalone dataset](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html), originally from the [UCI ML repository](https://archive.ics.uci.edu/ml/datasets/abalone). In this libsvm converted version, the nominal feature (Male/Female/Infant) has been converted into a real valued feature as required by XGBoost. The age of the abalone snail is predicted from eight physical measurements.  \n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "This notebook was tested in Amazon SageMaker Studio on a ml.t3.medium instance on the Python 3 (Data Science) kernel.\n",
    "\n",
    "Let's start by specifying:\n",
    "1. The S3 bucket and prefix to use for the training and model data. This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "1. The IAM role ARN used to grant access to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import sagemaker\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sagemaker.Session().boto_region_name\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"sagemaker/DEMO-xgboost-dist-script\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the dataset\n",
    "\n",
    "The following methods split the data into train/test/validation datasets and upload files to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import boto3\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "\n",
    "def data_split(FILE_DATA,\n",
    "               DATA_DIR,\n",
    "               FILE_TRAIN_BASE,\n",
    "               FILE_TRAIN_1,\n",
    "               FILE_TRAIN_2,\n",
    "               FILE_TRAIN_3,               \n",
    "               FILE_VALIDATION,\n",
    "               FILE_TEST,\n",
    "               PERCENT_TRAIN_0,\n",
    "               PERCENT_TRAIN_1,\n",
    "               PERCENT_TRAIN_2,\n",
    "               PERCENT_TRAIN_3,               \n",
    "               PERCENT_VALIDATION,\n",
    "               PERCENT_TEST,\n",
    "               ENLARGE\n",
    "              ):\n",
    "\n",
    "    data = [l for l in open(FILE_DATA, \"r\")]\n",
    "\n",
    "    print(-1, len(data))    \n",
    "\n",
    "    for i in range(ENLARGE):\n",
    "        data = data + data\n",
    "        print(i, len(data))        \n",
    "    \n",
    "    #print(type(data), len(data), data)\n",
    "    \n",
    "    print(1)\n",
    "    train_file_0 = open(DATA_DIR + \"/\" + FILE_TRAIN_0, \"w\")\n",
    "    print(2)    \n",
    "    train_file_1 = open(DATA_DIR + \"/\" + FILE_TRAIN_1, \"w\")\n",
    "    print(3)\n",
    "    train_file_2 = open(DATA_DIR + \"/\" + FILE_TRAIN_2, \"w\")\n",
    "    print(3)\n",
    "    train_file_3 = open(DATA_DIR + \"/\" + FILE_TRAIN_3, \"w\")\n",
    "    print(3)    \n",
    "    valid_file = open(DATA_DIR + \"/\" + FILE_VALIDATION, \"w\")\n",
    "    print(4)\n",
    "    tests_file = open(DATA_DIR + \"/\" + FILE_TEST, \"w\")\n",
    "\n",
    "    num_of_data = len(data)\n",
    "    num_train_0 = int((PERCENT_TRAIN_0 / 100.0) * num_of_data)\n",
    "    num_train_1 = int((PERCENT_TRAIN_1 / 100.0) * num_of_data)\n",
    "    num_train_2 = int((PERCENT_TRAIN_2 / 100.0) * num_of_data)\n",
    "    num_train_3 = int((PERCENT_TRAIN_3 / 100.0) * num_of_data)    \n",
    "    num_valid = int((PERCENT_VALIDATION / 100.0) * num_of_data)\n",
    "    num_tests = int((PERCENT_TEST / 100.0) * num_of_data)\n",
    "\n",
    "    data_fractions = [num_train_0, num_train_1, num_train_2, num_train_3, num_valid, num_tests]\n",
    "    split_data = [[], [], [], [], [], []]\n",
    "\n",
    "    rand_data_ind = 0\n",
    "\n",
    "    for split_ind, fraction in enumerate(data_fractions):\n",
    "        print(5)\n",
    "        for i in range(fraction):\n",
    "            #print(6)\n",
    "            rand_data_ind = random.randint(0, len(data) - 1)\n",
    "            split_data[split_ind].append(data[rand_data_ind])\n",
    "            data.pop(rand_data_ind)\n",
    "\n",
    "    for l in split_data[0]:\n",
    "        train_file_0.write(l)\n",
    "\n",
    "    for l in split_data[1]:\n",
    "        train_file_1.write(l)\n",
    "\n",
    "    for l in split_data[2]:\n",
    "        train_file_2.write(l)\n",
    "        \n",
    "    for l in split_data[3]:\n",
    "        train_file_3.write(l)        \n",
    "\n",
    "    for l in split_data[4]:\n",
    "        valid_file.write(l)\n",
    "\n",
    "    for l in split_data[5]:\n",
    "        tests_file.write(l)\n",
    "\n",
    "    train_file_0.close()\n",
    "    train_file_1.close()\n",
    "    train_file_2.close()\n",
    "    train_file_3.close()    \n",
    "    valid_file.close()\n",
    "    tests_file.close()\n",
    "\n",
    "def write_to_s3(fobj, bucket, key):\n",
    "    return (\n",
    "        boto3.Session(region_name=region)\n",
    "        .resource(\"s3\")\n",
    "        .Bucket(bucket)\n",
    "        .Object(key)\n",
    "        .upload_fileobj(fobj)\n",
    "    )\n",
    "\n",
    "def upload_to_s3(bucket, channel, filename):\n",
    "    fobj = open(filename, \"rb\")\n",
    "    key = prefix + \"/\" + channel\n",
    "    url = \"s3://{}/{}/{}\".format(bucket, key, filename)\n",
    "    print(\"Writing to {}\".format(url))\n",
    "    write_to_s3(fobj, bucket, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest data\n",
    "\n",
    "Next, we read the dataset from the existing repository into memory, for preprocessing prior to training. This processing could be done *in situ* by SageMaker Processing, Amazon Athena, Apache Spark in Amazon EMR, Amazon Redshift, etc., assuming the dataset is present in the appropriate location. Then, the next step would be to transfer the data to S3 for use in training. For small datasets, such as this one, reading into memory isn't onerous, though it would be for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# Load the dataset\n",
    "FILE_DATA = \"abalone\"\n",
    "s3.download_file(\n",
    "    \"sagemaker-sample-files\", f\"datasets/tabular/uci_abalone/abalone.libsvm\", FILE_DATA\n",
    ")\n",
    "\n",
    "# Split the downloaded data into train/test/validation files\n",
    "FILE_TRAIN_0 = \"abalone.train_0\"\n",
    "FILE_TRAIN_1 = \"abalone.train_1\"\n",
    "FILE_TRAIN_2 = \"abalone.train_2\"\n",
    "FILE_TRAIN_3 = \"abalone.train_3\"\n",
    "FILE_VALIDATION = \"abalone.validation\"\n",
    "FILE_TEST = \"abalone.test\"\n",
    "PERCENT_TRAIN_0 = 24\n",
    "PERCENT_TRAIN_1 = 24\n",
    "PERCENT_TRAIN_2 = 24\n",
    "PERCENT_TRAIN_3 = 24\n",
    "PERCENT_VALIDATION = 2\n",
    "PERCENT_TEST = 2\n",
    "ENLARGE = 10\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.mkdir(DATA_DIR)\n",
    "\n",
    "data_split(FILE_DATA,\n",
    "           DATA_DIR,\n",
    "           FILE_TRAIN_0,\n",
    "           FILE_TRAIN_1,\n",
    "           FILE_TRAIN_2,\n",
    "           FILE_TRAIN_3,           \n",
    "           FILE_VALIDATION,\n",
    "           FILE_TEST,\n",
    "           PERCENT_TRAIN_0,\n",
    "           PERCENT_TRAIN_1,\n",
    "           PERCENT_TRAIN_2,\n",
    "           PERCENT_TRAIN_3,           \n",
    "           PERCENT_VALIDATION,\n",
    "           PERCENT_TEST,\n",
    "           ENLARGE\n",
    "          )\n",
    "\n",
    "# Upload the files to the S3 bucket\n",
    "upload_to_s3(bucket, \"train/train_0.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_0)\n",
    "upload_to_s3(bucket, \"train/train_1.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_1)\n",
    "upload_to_s3(bucket, \"train/train_2.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_2)\n",
    "upload_to_s3(bucket, \"train/train_3.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_3)\n",
    "upload_to_s3(bucket, \"validation/validation.libsvm\", DATA_DIR + \"/\" + FILE_VALIDATION)\n",
    "upload_to_s3(bucket, \"test/test.libsvm\", DATA_DIR + \"/\" + FILE_TEST)\n",
    "\n",
    "print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_4.libsvm/data/abalone.train_4\n",
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_5.libsvm/data/abalone.train_5\n",
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_6.libsvm/data/abalone.train_6\n",
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_7.libsvm/data/abalone.train_7\n",
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_8.libsvm/data/abalone.train_8\n",
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_9.libsvm/data/abalone.train_9\n",
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_10.libsvm/data/abalone.train_10\n",
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_11.libsvm/data/abalone.train_11\n",
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_12.libsvm/data/abalone.train_12\n",
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_13.libsvm/data/abalone.train_13\n",
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_14.libsvm/data/abalone.train_14\n",
      "Writing to s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/train_15.libsvm/data/abalone.train_15\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"data\"\n",
    "\n",
    "FILE_TRAIN_4 = \"abalone.train_4\"\n",
    "FILE_TRAIN_5 = \"abalone.train_5\"\n",
    "FILE_TRAIN_6 = \"abalone.train_6\"\n",
    "FILE_TRAIN_7 = \"abalone.train_7\"\n",
    "FILE_TRAIN_8 = \"abalone.train_8\"\n",
    "FILE_TRAIN_9 = \"abalone.train_9\"\n",
    "FILE_TRAIN_10 = \"abalone.train_10\"\n",
    "FILE_TRAIN_11 = \"abalone.train_11\"\n",
    "FILE_TRAIN_12 = \"abalone.train_12\"\n",
    "FILE_TRAIN_13 = \"abalone.train_13\"\n",
    "FILE_TRAIN_14 = \"abalone.train_14\"\n",
    "FILE_TRAIN_15 = \"abalone.train_15\"\n",
    "\n",
    "upload_to_s3(bucket, \"train/train_4.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_4)\n",
    "upload_to_s3(bucket, \"train/train_5.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_5)\n",
    "upload_to_s3(bucket, \"train/train_6.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_6)\n",
    "upload_to_s3(bucket, \"train/train_7.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_7)\n",
    "upload_to_s3(bucket, \"train/train_8.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_8)\n",
    "upload_to_s3(bucket, \"train/train_9.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_9)\n",
    "upload_to_s3(bucket, \"train/train_10.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_10)\n",
    "upload_to_s3(bucket, \"train/train_11.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_11)\n",
    "upload_to_s3(bucket, \"train/train_12.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_12)\n",
    "upload_to_s3(bucket, \"train/train_13.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_13)\n",
    "upload_to_s3(bucket, \"train/train_14.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_14)\n",
    "upload_to_s3(bucket, \"train/train_15.libsvm\", DATA_DIR + \"/\" + FILE_TRAIN_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create an XGBoost training script\n",
    "\n",
    "SageMaker can now run an XGBoost script using the XGBoost estimator. When run on SageMaker, a number of helpful environment variables are available to access properties of the training environment, such as:\n",
    "\n",
    "- `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to. Any artifacts saved in this folder are uploaded to S3 for model hosting after the training job completes.\n",
    "- `SM_OUTPUT_DIR`: A string representing the filesystem path to write output artifacts to. Output artifacts may include checkpoints, graphs, and other files to save, not including model artifacts. These artifacts are compressed and uploaded to S3 to the same S3 prefix as the model artifacts.\n",
    "\n",
    "When two input channels, `train` and `validation`, are used in the call to the XGBoost estimator's `fit()` method, the following environment variables are set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "- `SM_CHANNEL_TRAIN`: A string representing the path to the directory containing data in the 'train' channel.\n",
    "- `SM_CHANNEL_VALIDATION`: Same as above, but for the 'validation' channel.\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to the `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance. For example, the script that we run in this notebook is provided as the accompanying file (`abalone.py`) and also shown below:\n",
    "\n",
    "```python\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "from sagemaker_containers import entry_point\n",
    "from sagemaker_xgboost_container.data_utils import get_dmatrix\n",
    "from sagemaker_xgboost_container import distributed\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "def _xgb_train(params, dtrain, evals, num_boost_round, model_dir, is_master):\n",
    "    \"\"\"Run xgb train on arguments given with rabit initialized.\n",
    "\n",
    "    This is our rabit execution function.\n",
    "\n",
    "    :param args_dict: Argument dictionary used to run xgb.train().\n",
    "    :param is_master: True if current node is master host in distributed training,\n",
    "                        or is running single node training job.\n",
    "                        Note that rabit_run includes this argument.\n",
    "    \"\"\"\n",
    "    booster = xgb.train(params=params,\n",
    "                        dtrain=dtrain,\n",
    "                        evals=evals,\n",
    "                        num_boost_round=num_boost_round)\n",
    "\n",
    "    if is_master:\n",
    "        model_location = model_dir + '/xgboost-model'\n",
    "        pkl.dump(booster, open(model_location, 'wb'))\n",
    "        logging.info(\"Stored trained model at {}\".format(model_location))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here.\n",
    "    parser.add_argument('--max_depth', type=int,)\n",
    "    parser.add_argument('--eta', type=float)\n",
    "    parser.add_argument('--gamma', type=int)\n",
    "    parser.add_argument('--min_child_weight', type=int)\n",
    "    parser.add_argument('--subsample', type=float)\n",
    "    parser.add_argument('--verbosity', type=int)\n",
    "    parser.add_argument('--objective', type=str)\n",
    "    parser.add_argument('--num_round', type=int)\n",
    "    parser.add_argument('--tree_method', type=str, default=\"auto\")\n",
    "    parser.add_argument('--predictor', type=str, default=\"auto\")\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output_data_dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    parser.add_argument('--sm_hosts', type=str, default=os.environ.get('SM_HOSTS'))\n",
    "    parser.add_argument('--sm_current_host', type=str, default=os.environ.get('SM_CURRENT_HOST'))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Get SageMaker host information from runtime environment variables\n",
    "    sm_hosts = json.loads(args.sm_hosts)\n",
    "    sm_current_host = args.sm_current_host\n",
    "\n",
    "    dtrain = get_dmatrix(args.train, 'libsvm')\n",
    "    dval = get_dmatrix(args.validation, 'libsvm')\n",
    "    watchlist = [(dtrain, 'train'), (dval, 'validation')] if dval is not None else [(dtrain, 'train')]\n",
    "\n",
    "    train_hp = {'max_depth': args.max_depth,\n",
    "                'eta': args.eta,\n",
    "                'gamma': args.gamma,\n",
    "                'min_child_weight': args.min_child_weight,\n",
    "                'subsample': args.subsample,\n",
    "                'verbosity': args.verbosity,\n",
    "                'objective': args.objective,\n",
    "                'tree_method': args.tree_method,\n",
    "                'predictor': args.predictor,\n",
    "               }\n",
    "\n",
    "    xgb_train_args = dict(params=train_hp,\n",
    "                          dtrain=dtrain,\n",
    "                          evals=watchlist,\n",
    "                          num_boost_round=args.num_round,\n",
    "                          model_dir=args.model_dir\n",
    "                         )\n",
    "\n",
    "    if len(sm_hosts) > 1:\n",
    "        # Wait until all hosts are able to find each other\n",
    "        entry_point._wait_hostname_resolution()\n",
    "\n",
    "        # Execute training function after initializing rabit.\n",
    "        distributed.rabit_run(exec_fun=_xgb_train,\n",
    "                              args=xgb_train_args,\n",
    "                              include_in_training=(dtrain is not None),\n",
    "                              hosts=sm_hosts,\n",
    "                              current_host=sm_current_host,\n",
    "                              update_rabit_args=True\n",
    "                             )\n",
    "    else:\n",
    "        # If single node training, call training method directly.\n",
    "        if dtrain:\n",
    "            xgb_train_args['is_master'] = True\n",
    "            _xgb_train(**xgb_train_args)\n",
    "        else:\n",
    "            raise ValueError(\"Training channel must have data to train model.\")\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize and return fitted model.\n",
    "\n",
    "    Note that this should have the same name as the serialized model in the _xgb_train method\n",
    "    \"\"\"\n",
    "    model_file = 'xgboost-model'\n",
    "    booster = pkl.load(open(os.path.join(model_dir, model_file), 'rb'))\n",
    "    return booster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the container imports your training script, always put your training code in a main guard `(if __name__=='__main__':)` so that the container does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For more information about training environment variables, please visit the [SageMaker Training Toolkit](https://github.com/aws/sagemaker-training-toolkit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the XGBoost model\n",
    "\n",
    "After setting training parameters, we kick off training, and poll for status until training is complete.\n",
    "\n",
    "To run our training script on SageMaker, we construct a sagemaker.xgboost.estimator.XGBoost estimator, which accepts several constructor arguments:\n",
    "\n",
    "* __entry_point__: The path to the Python script that SageMaker runs for training and prediction.\n",
    "* __role__: Role ARN\n",
    "* __train_instance_type__ *(optional)*: The type of SageMaker instances for training.\n",
    "* __sagemaker_session__ *(optional)*: The session used to train on SageMaker.\n",
    "* __hyperparameters__ *(optional)*: A dictionary passed to the train function as hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {#\"tree_method\": \"gpu_hist\",\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"50\",\n",
    "    \"verbosity\": \"2\",\n",
    "}\n",
    "\n",
    "instance_type = \"ml.m5.2xlarge\"\n",
    "output_path = \"s3://{}/{}/{}/output\".format(bucket, prefix, \"abalone-dist-xgb\")\n",
    "content_type = \"libsvm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.c5.4xlarge.\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "########### Use XGBoost as a framework ##########\n",
    "#################################################\n",
    "\n",
    "### Open Source distributed script mode ###\n",
    "\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "session = Session()\n",
    "script_path = \"abalone.py\"\n",
    "\n",
    "instance_type = \"ml.c5.4xlarge\" #\"ml.m5.2xlarge\", \"ml.p3.2xlarge\", \"ml.p3.16xlarge\"\n",
    "\n",
    "\"\"\"\n",
    "ml.c5.9xlarge, ml.c5.xlarge, ml.c4.xlarge, ml.c5.18xlarge, ml.m5.xlarge, ml.m4.10xlarge, ml.m5.12xlarge, ml.m4.xlarge, ml.m5.24xlarge, ml.m4.2xlarge, ml.m5.2xlarge,  \n",
    "ml.m4.4xlarge\n",
    "\"\"\"\n",
    "\n",
    "xgb_script_mode_estimator = XGBoost(entry_point=script_path,\n",
    "                                    framework_version=\"1.7-1\",  # Note: framework_version is mandatory\n",
    "                                    hyperparameters=hyperparams,\n",
    "                                    role=role,\n",
    "                                    instance_count=1,\n",
    "                                    instance_type=instance_type,\n",
    "                                    output_path=output_path,\n",
    "                                   )\n",
    "\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}/\".format(bucket, prefix, \"train\"), \n",
    "                            content_type=content_type,\n",
    "                            input_mode = \"Pipe\", # \"File\", \"FastFile\", \"Pipe\"--(Possibly only for: Parquet and Recordio-protobuf input formats)\n",
    "                            distribution = \"ShardedByS3Key\", #\"FullyReplicated\", # \"ShardedByS3Key\",\n",
    "                           )\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}/\".format(bucket, prefix, \"validation\"), \n",
    "                                 content_type=content_type                                 \n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-578864530451/sagemaker/DEMO-xgboost-dist-script/train/'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"s3://{}/{}/{}/\".format(bucket, prefix, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train an XGBoost Estimator on the Abalone data \n",
    "\n",
    "\n",
    "Training is as simple as calling `fit()` on the Estimator. This starts a SageMaker training job that downloads the data, invokes the entry point code (in the provided script file), and saves any model artifacts that the script creates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-05-11-17-42-55-612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "2023-05-11 17:42:55 Starting - Starting the training job...\n",
      "2023-05-11 17:43:13 Starting - Preparing the instances for training......\n",
      "2023-05-11 17:44:12 Downloading - Downloading input data...\n",
      "2023-05-11 17:44:42 Training - Training image download completed. Training in progress...\u001b[34m[2023-05-11 17:45:01.077 ip-10-2-96-228.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-11 17:45:01.137 ip-10-2-96-228.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-11:17:45:01:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2023-05-11:17:45:01:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-05-11:17:45:01:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[34m[2023-05-11:17:45:01:INFO] Module abalone does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m[2023-05-11:17:45:01:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[34m[2023-05-11:17:45:01:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m[2023-05-11:17:45:01:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: abalone\n",
      "  Building wheel for abalone (setup.py): started\n",
      "  Building wheel for abalone (setup.py): finished with status 'done'\n",
      "  Created wheel for abalone: filename=abalone-1.0.0-py2.py3-none-any.whl size=6615 sha256=854b73f5fef998db8fa9105939b35de3ec28db2499a23c989219c3a1cbfa07a5\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-qobfltgw/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[34mSuccessfully built abalone\u001b[0m\n",
      "\u001b[34mInstalling collected packages: abalone\u001b[0m\n",
      "\u001b[34mSuccessfully installed abalone-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[2023-05-11:17:45:03:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-05-11:17:45:03:INFO] Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"validation\": \"/opt/ml/input/data/validation\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"eta\": \"0.2\",\n",
      "        \"gamma\": \"4\",\n",
      "        \"max_depth\": \"5\",\n",
      "        \"min_child_weight\": \"6\",\n",
      "        \"num_round\": \"50\",\n",
      "        \"objective\": \"reg:squarederror\",\n",
      "        \"subsample\": \"0.7\",\n",
      "        \"verbosity\": \"2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"libsvm\",\n",
      "            \"TrainingInputMode\": \"Pipe\",\n",
      "            \"S3DistributionType\": \"ShardedByS3Key\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"libsvm\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-xgboost-2023-05-11-17-42-55-612\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-578864530451/sagemaker-xgboost-2023-05-11-17-42-55-612/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"abalone\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 16,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.4xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.4xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"abalone.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=abalone.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.4xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"ShardedByS3Key\",\"TrainingInputMode\":\"Pipe\"},\"validation\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=abalone\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=16\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-578864530451/sagemaker-xgboost-2023-05-11-17-42-55-612/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"eta\":\"0.2\",\"gamma\":\"4\",\"max_depth\":\"5\",\"min_child_weight\":\"6\",\"num_round\":\"50\",\"objective\":\"reg:squarederror\",\"subsample\":\"0.7\",\"verbosity\":\"2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"ShardedByS3Key\",\"TrainingInputMode\":\"Pipe\"},\"validation\":{\"ContentType\":\"libsvm\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-xgboost-2023-05-11-17-42-55-612\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-578864530451/sagemaker-xgboost-2023-05-11-17-42-55-612/source/sourcedir.tar.gz\",\"module_name\":\"abalone\",\"network_interface_name\":\"eth0\",\"num_cpus\":16,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.4xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.4xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"abalone.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--eta\",\"0.2\",\"--gamma\",\"4\",\"--max_depth\",\"5\",\"--min_child_weight\",\"6\",\"--num_round\",\"50\",\"--objective\",\"reg:squarederror\",\"--subsample\",\"0.7\",\"--verbosity\",\"2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_HP_ETA=0.2\u001b[0m\n",
      "\u001b[34mSM_HP_GAMMA=4\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_DEPTH=5\u001b[0m\n",
      "\u001b[34mSM_HP_MIN_CHILD_WEIGHT=6\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_ROUND=50\u001b[0m\n",
      "\u001b[34mSM_HP_OBJECTIVE=reg:squarederror\u001b[0m\n",
      "\u001b[34mSM_HP_SUBSAMPLE=0.7\u001b[0m\n",
      "\u001b[34mSM_HP_VERBOSITY=2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m abalone --eta 0.2 --gamma 4 --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/miniconda3/lib/python3.8/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/ml/code/abalone.py\", line 81, in <module>\n",
      "    dtrain = get_dmatrix(args.train, \"libsvm\")\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/sagemaker_xgboost_container/data_utils.py\", line 600, in get_dmatrix\n",
      "    if dmatrix and dmatrix.get_label().size == 0:\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/core.py\", line 1006, in get_label\n",
      "    return self.get_float_info('label')\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/core.py\", line 855, in get_float_info\n",
      "    _check_call(_LIB.XGDMatrixGetFloatInfo(self.handle,\n",
      "  File \"/miniconda3/lib/python3.8/site-packages/xgboost/core.py\", line 279, in _check_call\n",
      "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\u001b[0m\n",
      "\u001b[34mxgboost.core.XGBoostError: [17:45:03] ../src/c_api/c_api.cc:649: DMatrix/Booster has not been initialized or has already been disposed.\u001b[0m\n",
      "\u001b[34mStack trace:\n",
      "  [bt] (0) /miniconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(+0x128959) [0x7f8d0343f959]\n",
      "  [bt] (1) /miniconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.so(XGDMatrixGetFloatInfo+0xba) [0x7f8d03440d5a]\n",
      "  [bt] (2) /miniconda3/lib/python3.8/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f8d747549dd]\n",
      "  [bt] (3) /miniconda3/lib/python3.8/lib-dynload/../../libffi.so.7(+0x6067) [0x7f8d74754067]\n",
      "  [bt] (4) /miniconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(_ctypes_callproc+0x319) [0x7f8d48f6b1e9]\n",
      "  [bt] (5) /miniconda3/lib/python3.8/lib-dynload/_ctypes.cpython-38-x86_64-linux-gnu.so(+0x13c95) [0x7f8d48f6bc95]\n",
      "  [bt] (6) /miniconda3/bin/python3(_PyObject_MakeTpCall+0x3bf) [0x5587429d513f]\n",
      "  [bt] (7) /miniconda3/bin/python3(_PyEval_EvalFrameDefault+0x5434) [0x558742a7fdd4]\n",
      "  [bt] (8) /miniconda3/bin/python3(_PyFunction_Vectorcall+0x1b7) [0x558742a717e7]\u001b[0m\n",
      "\u001b[34m[2023-05-11:17:45:03:ERROR] ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/miniconda3/bin/python3 -m abalone --eta 0.2 --gamma 4 --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\"\u001b[0m\n",
      "\n",
      "2023-05-11 17:45:14 Uploading - Uploading generated training model\n",
      "2023-05-11 17:45:14 Failed - Training job failed\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job sagemaker-xgboost-2023-05-11-17-42-55-612: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python3 -m abalone --eta 0.2 --gamma 4 --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-8cd04700425b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mxgb_script_mode_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalidation_input\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TRAIN TIME OUTSIDE\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/workflow/pipeline_context.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_StepArguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_caller_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_instance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   2342\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2344\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2345\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2346\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   4628\u001b[0m             \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnexpectedStatusException\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mwaiting\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mjob\u001b[0m \u001b[0mfails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4629\u001b[0m         \"\"\"\n\u001b[0;32m-> 4630\u001b[0;31m         \u001b[0m_logs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4632\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlogs_for_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_logs_for_job\u001b[0;34m(boto_session, job_name, wait, poll, log_type, timeout)\u001b[0m\n\u001b[1;32m   6490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6491\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6492\u001b[0;31m         \u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6493\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6494\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   6546\u001b[0m             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6547\u001b[0m             \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6548\u001b[0;31m             \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6549\u001b[0m         )\n\u001b[1;32m   6550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job sagemaker-xgboost-2023-05-11-17-42-55-612: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/miniconda3/bin/python3 -m abalone --eta 0.2 --gamma 4 --max_depth 5 --min_child_weight 6 --num_round 50 --objective reg:squarederror --subsample 0.7 --verbosity 2\", exit code: 1"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "xgb_script_mode_estimator.fit({\"train\": train_input, \"validation\": validation_input})\n",
    "print(\"TRAIN TIME OUTSIDE\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File_Size^1 \n",
    "\n",
    "instance_count=2, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"FullyReplicated\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 195.38521814346313 TRAIN TIME INSIDE.PY 6.88726019859314\n",
    "\n",
    "instance_count=1, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"FullyReplicated\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 253.56789135932922 TRAIN TIME INSIDE.PY 0.1523737907409668\n",
    "\n",
    "instance_count=2, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 226.4941885471344 TRAIN TIME INSIDE.PY 3.7223174571990967\n",
    "TRAIN TIME OUTSIDE 317.8059356212616 TRAIN TIME INSIDE.PY 3.8782737255096436\n",
    "\n",
    "instance_count=1, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 254.0368688106537 TRAIN TIME INSIDE.PY 0.22278976440429688\n",
    "\n",
    "# File_Size^9\n",
    "instance_count=2, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 195.24812078475952 TRAIN TIME INSIDE.PY 9.903223037719727\n",
    "\n",
    "instance_count=1, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 224.5310173034668 TRAIN TIME INSIDE.PY 49.18486475944519\n",
    "\n",
    "instance_count=4, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 199.1698350906372 TRAIN TIME INSIDE.PY 10.55151915550232\n",
    "\n",
    "# File_Size^10\n",
    "\n",
    "instance_count=4, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 201.93874502182007, TRAIN TIME INSIDE.PY 14.228670835494995 algo-4\n",
    "\n",
    "instance_count=2, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 195.4813358783722, TRAIN TIME INSIDE.PY 16.20287251472473 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 288.803368806839, TRAIN TIME INSIDE.PY 116.7438588142395 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.m5.xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 318.66870307922363, TRAIN TIME INSIDE.PY 163.08214235305786 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.m5.4xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 256.3569004535675, TRAIN TIME INSIDE.PY 78.06328439712524 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.c5.4xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 255.56110048294067, TRAIN TIME INSIDE.PY 68.70053768157959 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.c4.8xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 321.32916283607483, TRAIN TIME INSIDE.PY 113.5972547531128 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.c5.9xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 259.23794627189636, TRAIN TIME INSIDE.PY 76.00906419754028 algo-1\n",
    "\n",
    "instance_count=2, instance_type = \"ml.c5.4xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "\n",
    "\n",
    "instance_count=2, instance_type = \"ml.c5.4xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 196.1646420955658, TRAIN TIME INSIDE.PY 16.606664419174194 algo-2\n",
    "\n",
    "instance_count=4, instance_type = \"ml.c5.4xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 198.0961720943451, TRAIN TIME INSIDE.PY 16.196258783340454 algo-3\n",
    "\n",
    "instance_count=1, instance_type = \"ml.p3.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 4 Files\n",
    "TRAIN TIME OUTSIDE 283.93668007850647, TRAIN TIME INSIDE.PY 1.416290044784546 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.p3.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 16 Files\n",
    "TRAIN TIME OUTSIDE 224.08697986602783 TRAIN TIME INSIDE.PY 2.3939082622528076 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.p3.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "TRAIN TIME OUTSIDE 256.29279017448425, TRAIN TIME INSIDE.PY 4.323373556137085 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.p3.16xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "TRAIN TIME OUTSIDE 256.29279017448425, TRAIN TIME INSIDE.PY 4.410764455795288 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.c5.4xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "TRAIN TIME OUTSIDE xxx.xxxxxxxxxxxxx, TRAIN TIME INSIDE.PY 166.97041201591492 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.c5.4xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"FastFile\", 32 Files\n",
    "TRAIN TIME OUTSIDE 385.2515449523926, TRAIN TIME INSIDE.PY 172.4998059272766 algo-1\n",
    "\n",
    "instance_count=1, instance_type = \"ml.c5.4xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"Pipe\", 32 Files\n",
    "--FAILED-- Possibly \"PIPE\" works only with Parquet and Recordio-protobuf input formats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2023-05-12-17-03-26-283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-12 17:03:26 Starting - Starting the training job...\n",
      "2023-05-12 17:03:42 Starting - Preparing the instances for training......\n",
      "2023-05-12 17:04:37 Downloading - Downloading input data...\n",
      "2023-05-12 17:05:22 Training - Training image download completed. Training in progress....\u001b[34m[2023-05-12 17:05:42.305 ip-10-0-72-219.ec2.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-05-12 17:05:42.364 ip-10-0-72-219.ec2.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-05-12:17:05:42:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2023-05-12:17:05:42:INFO] Failed to parse hyperparameter objective value reg:squarederror to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m[2023-05-12:17:05:42:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2023-05-12:17:05:42:INFO] Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34m[2023-05-12:17:05:42:INFO] Determined 0 GPU(s) available on the instance.\u001b[0m\n",
      "\u001b[34m[2023-05-12:17:05:42:INFO] files path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34m[2023-05-12:17:05:58:INFO] files path: /opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34m[2023-05-12:17:05:58:INFO] Single node training.\u001b[0m\n",
      "\u001b[34m[2023-05-12:17:05:58:INFO] Train matrix has 32841970 rows and 9 columns\u001b[0m\n",
      "\u001b[34m[2023-05-12:17:05:58:INFO] Validation matrix has 85520 rows\u001b[0m\n",
      "\u001b[34m[2023-05-12 17:05:58.932 ip-10-0-72-219.ec2.internal:7 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-05-12 17:05:58.932 ip-10-0-72-219.ec2.internal:7 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-05-12 17:05:58.933 ip-10-0-72-219.ec2.internal:7 INFO hook.py:259] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-05-12 17:05:58.933 ip-10-0-72-219.ec2.internal:7 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-05-12:17:05:58:INFO] Debug hook created from config\u001b[0m\n",
      "\u001b[34m[17:05:59] INFO: ../src/gbm/gbtree.cc:149: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\u001b[0m\n",
      "\u001b[34m[17:06:00] INFO: ../src/data/simple_dmatrix.cc:103: Generating new Gradient Index.\u001b[0m\n",
      "\u001b[34m[2023-05-12 17:06:29.429 ip-10-0-72-219.ec2.internal:7 INFO hook.py:427] Monitoring the collections: metrics\u001b[0m\n",
      "\u001b[34m[2023-05-12 17:06:29.432 ip-10-0-72-219.ec2.internal:7 INFO hook.py:491] Hook is writing from the hook with pid: 7\u001b[0m\n",
      "\u001b[34m[0]#011train-rmse:8.08324#011validation-rmse:8.07999\u001b[0m\n",
      "\u001b[34m[1]#011train-rmse:6.59391#011validation-rmse:6.59321\u001b[0m\n",
      "\u001b[34m[2]#011train-rmse:5.42113#011validation-rmse:5.42098\u001b[0m\n",
      "\u001b[34m[3]#011train-rmse:4.51196#011validation-rmse:4.51342\u001b[0m\n",
      "\u001b[34m[4]#011train-rmse:3.81076#011validation-rmse:3.81325\u001b[0m\n",
      "\u001b[34m[5]#011train-rmse:3.28529#011validation-rmse:3.28916\u001b[0m\n",
      "\u001b[34m[6]#011train-rmse:2.90020#011validation-rmse:2.90606\u001b[0m\n",
      "\u001b[34m[7]#011train-rmse:5.68107#011validation-rmse:6.57380\u001b[0m\n",
      "\u001b[34m[8]#011train-rmse:3.60351#011validation-rmse:4.01292\u001b[0m\n",
      "\u001b[34m[9]#011train-rmse:3.05714#011validation-rmse:3.34578\u001b[0m\n",
      "\u001b[34m[10]#011train-rmse:2.56176#011validation-rmse:2.72149\u001b[0m\n",
      "\u001b[34m[11]#011train-rmse:2.28540#011validation-rmse:2.37252\u001b[0m\n",
      "\u001b[34m[12]#011train-rmse:2.15706#011validation-rmse:2.21484\u001b[0m\n",
      "\u001b[34m[13]#011train-rmse:2.06280#011validation-rmse:2.09391\u001b[0m\n",
      "\u001b[34m[14]#011train-rmse:2.04011#011validation-rmse:2.03957\u001b[0m\n",
      "\u001b[34m[15]#011train-rmse:3.48934#011validation-rmse:3.13641\u001b[0m\n",
      "\u001b[34m[16]#011train-rmse:9.47581#011validation-rmse:8.06913\u001b[0m\n",
      "\u001b[34m[17]#011train-rmse:45.90586#011validation-rmse:38.70782\u001b[0m\n",
      "\u001b[34m[18]#011train-rmse:217.94328#011validation-rmse:183.69847\u001b[0m\n",
      "\u001b[34m[19]#011train-rmse:691.93409#011validation-rmse:583.20295\u001b[0m\n",
      "\u001b[34m[20]#011train-rmse:6004.75767#011validation-rmse:5061.15579\u001b[0m\n",
      "\u001b[34m[21]#011train-rmse:26588.31159#011validation-rmse:22410.16067\u001b[0m\n",
      "\u001b[34m[22]#011train-rmse:146335.12187#011validation-rmse:123339.67044\u001b[0m\n",
      "\u001b[34m[23]#011train-rmse:531005.90908#011validation-rmse:447562.36909\u001b[0m\n",
      "\u001b[34m[24]#011train-rmse:1436491.07951#011validation-rmse:1210757.43175\u001b[0m\n",
      "\u001b[34m[25]#011train-rmse:8158905.85353#011validation-rmse:6876795.85205\u001b[0m\n",
      "\u001b[34m[26]#011train-rmse:33710211.72953#011validation-rmse:28412908.34273\u001b[0m\n",
      "\u001b[34m[27]#011train-rmse:159720455.33343#011validation-rmse:134621600.54817\u001b[0m\n",
      "\u001b[34m[28]#011train-rmse:868761654.56308#011validation-rmse:732242368.00482\u001b[0m\n",
      "\u001b[34m[29]#011train-rmse:3233680480.82981#011validation-rmse:2725532187.35767\u001b[0m\n",
      "\u001b[34m[30]#011train-rmse:15359099242.63919#011validation-rmse:12945533611.87113\u001b[0m\n",
      "\u001b[34m[31]#011train-rmse:66028765686.44244#011validation-rmse:55652847347.40245\u001b[0m\n",
      "\u001b[34m[32]#011train-rmse:167951222948.01947#011validation-rmse:141558965632.07870\u001b[0m\n",
      "\u001b[34m[33]#011train-rmse:435315997598.72626#011validation-rmse:366909399416.78363\u001b[0m\n",
      "\u001b[34m[34]#011train-rmse:1270867631094.40015#011validation-rmse:1071160448582.69922\u001b[0m\n",
      "\u001b[34m[35]#011train-rmse:7123253166916.45020#011validation-rmse:6003888108371.83887\u001b[0m\n",
      "\u001b[34m[36]#011train-rmse:22230032363567.21484#011validation-rmse:18736751850435.50000\u001b[0m\n",
      "\u001b[34m[37]#011train-rmse:50646403353772.38281#011validation-rmse:42687706263168.97656\u001b[0m\n",
      "\u001b[34m[38]#011train-rmse:154722837209586.65625#011validation-rmse:130409320102589.12500\u001b[0m\n",
      "\u001b[34m[39]#011train-rmse:154716372700565.43750#011validation-rmse:130404120023543.92188\u001b[0m\n",
      "\u001b[34m[40]#011train-rmse:154712265694480.68750#011validation-rmse:130401212308522.70312\u001b[0m\n",
      "\u001b[34m[41]#011train-rmse:154709651428223.59375#011validation-rmse:130399674806711.12500\u001b[0m\n",
      "\u001b[34m[42]#011train-rmse:154707984568890.75000#011validation-rmse:130398944601180.25000\u001b[0m\n",
      "\u001b[34m[43]#011train-rmse:154706939798509.56250#011validation-rmse:130398680251995.15625\u001b[0m\n",
      "\u001b[34m[44]#011train-rmse:154706155279702.21875#011validation-rmse:130398594558187.81250\u001b[0m\n",
      "\u001b[34m[45]#011train-rmse:154705018165024.78125#011validation-rmse:130398162726070.73438\u001b[0m\n",
      "\u001b[34m[46]#011train-rmse:154703597576584.75000#011validation-rmse:130397481781444.50000\u001b[0m\n",
      "\u001b[34m[47]#011train-rmse:154701858389340.25000#011validation-rmse:130396556789556.48438\u001b[0m\n",
      "\u001b[34m[48]#011train-rmse:154699768384669.21875#011validation-rmse:130395338050422.18750\u001b[0m\n",
      "\n",
      "2023-05-12 17:09:34 Uploading - Uploading generated training model\u001b[34m[49]#011train-rmse:154697423747292.06250#011validation-rmse:130393951793941.46875\u001b[0m\n",
      "\n",
      "2023-05-12 17:09:45 Completed - Training job completed\n",
      "Training seconds: 308\n",
      "Billable seconds: 308\n",
      "TRAIN TIME OUTSIDE 419.74195885658264\n"
     ]
    }
   ],
   "source": [
    "#################################################\n",
    "###### Use XGBoost as a built-in algorithm ######\n",
    "#################################################\n",
    "\n",
    "import sagemaker, time\n",
    "import boto3\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "# initialize hyperparameters\n",
    "hyperparams = {#\"tree_method\": \"gpu_hist\",\n",
    "    \"max_depth\": \"5\",\n",
    "    \"eta\": \"0.2\",\n",
    "    \"gamma\": \"4\",\n",
    "    \"min_child_weight\": \"6\",\n",
    "    \"subsample\": \"0.7\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"50\",\n",
    "    \"verbosity\": \"2\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# set an output path where the trained model will be saved\n",
    "bucket = sagemaker.Session().default_bucket()\n",
    "prefix = \"sagemaker/DEMO-xgboost-dist-script\"\n",
    "output_path = 's3://{}/{}/{}/output'.format(bucket, prefix, 'abalone-xgb-built-in-algo')\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.7-1\")\n",
    "\n",
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=xgboost_container, \n",
    "                                          hyperparameters=hyperparams,\n",
    "                                          role=sagemaker.get_execution_role(),\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge', #'ml.m5.2xlarge', 'ml.c5.2xlarge', ml.p3.16xlarge\n",
    "                                          volume_size=5, # 5 GB \n",
    "                                          output_path=output_path)\n",
    "\n",
    "# define the data type and paths to the training and validation datasets\n",
    "content_type = \"libsvm\"\n",
    "train_input = TrainingInput(\"s3://{}/{}/{}/\".format(bucket, prefix, 'train'),\n",
    "                            input_mode = \"File\", # \"File\", \"FastFile\", \"Pipe\"--(Possibly only for: Parquet and Recordio-protobuf input formats)\n",
    "                            distribution = \"FullyReplicated\", #\"FullyReplicated\", # \"ShardedByS3Key\",\n",
    "                            content_type=content_type)\n",
    "\n",
    "validation_input = TrainingInput(\"s3://{}/{}/{}/\".format(bucket, prefix, 'validation'), \n",
    "                                 content_type=content_type)\n",
    "\n",
    "# execute the XGBoost training job\n",
    "\n",
    "start_time = time.time()\n",
    "estimator.fit({'train': train_input, 'validation': validation_input})\n",
    "print(\"TRAIN TIME OUTSIDE\", time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_count=1, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "[2023-05-11:20:11:42:INFO] @tracker !?!?! NOT AVAIlABLE WITH SINGLE INSTANCE !?!?! secs between node start and job finish\n",
    "Billable seconds: 316/1\n",
    "TRAIN TIME OUTSIDE 415.39980149269104\n",
    "\n",
    "instance_count=2, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "[2023-05-11:20:11:42:INFO] @tracker 107.9842574596405 secs between node start and job finish\n",
    "Billable seconds: 406/2\n",
    "TRAIN TIME OUTSIDE 355.3289113044739\n",
    "\n",
    "instance_count=1, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"FullyReplicated\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "[2023-05-11:20:11:42:INFO] @tracker !?!?! NOT AVAIlABLE WITH SINGLE INSTANCE !?!?! secs between node start and job finish\n",
    "Billable seconds: 308\n",
    "TRAIN TIME OUTSIDE 419.74195885658264\n",
    "\n",
    "instance_count=2, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"FullyReplicated\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "[2023-05-12:16:57:47:INFO] @tracker 216.17183685302734 secs between node start and job finish\n",
    "Billable seconds: 616\n",
    "TRAIN TIME OUTSIDE 399.7200503349304\n",
    "\n",
    "instance_count=2, instance_type = \"ml.m5.xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "[2023-05-11:20:27:31:INFO] @tracker 134.74564719200134 secs between node start and job finish\n",
    "Billable seconds: 456/2\n",
    "TRAIN TIME OUTSIDE 327.17588686943054\n",
    "\n",
    "instance_count=2, instance_type = \"ml.m5.4xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "[2023-05-11:20:34:05:INFO] @tracker 92.06404709815979 secs between node start and job finish\n",
    "Billable seconds: 356/2\n",
    "TRAIN TIME OUTSIDE 292.1142199039459\n",
    "\n",
    "instance_count=4, instance_type = \"ml.m5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "[2023-05-11:20:40:18:INFO] @tracker 54.68413758277893 secs between node start and job finish\n",
    "Billable seconds: 572/4\n",
    "TRAIN TIME OUTSIDE 271.30717182159424\n",
    "\n",
    "instance_count=4, instance_type = \"ml.c5.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "[2023-05-11:20:48:53:INFO] @tracker 49.115756034851074 secs between node start and job finish\n",
    "Billable seconds: 592/4\n",
    "TRAIN TIME OUTSIDE 268.46313285827637\n",
    "\n",
    "instance_count=1, instance_type = \"ml.p3.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "[2023-05-11:20:11:42:INFO] @tracker !?!?! NOT AVAIlABLE WITH SINGLE INSTANCE !?!?! secs between node start and job finish\n",
    "Billable seconds: 123/1\n",
    "TRAIN TIME OUTSIDE 253.56209897994995\n",
    "\n",
    "instance_count=1, instance_type = \"ml.p3.16xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"File\", 32 Files\n",
    "[2023-05-11:20:11:42:INFO] @tracker !?!?! NOT AVAIlABLE WITH SINGLE INSTANCE !?!?! secs between node start and job finish\n",
    "Billable seconds: 103/1\n",
    "TRAIN TIME OUTSIDE 345.8657298088074\n",
    "\n",
    "instance_count=1, instance_type = \"ml.p3.2xlarge\", \"S3DistributionType\":\"ShardedByS3Key\", \"TrainingInputMode\":\"Pipe\", 32 Files\n",
    "--FAILED-- probabaly Pipe mode odes not work with the data format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the XGBoost model\n",
    "\n",
    "After training, we use the estimator to create an Amazon SageMaker endpoint  a hosted and managed prediction service to perform inference.\n",
    "\n",
    "You can also optionally specify other functions to customize the deserialization of the input request (`input_fn()`), serialization of the predictions (`output_fn()`), and how predictions are made (`predict_fn()`). The defaults work for our current use-case so we dont need to define them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = xgb_script_mode_estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.2xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = DATA_DIR + \"/\" + FILE_TEST\n",
    "with open(test_file, \"r\") as f:\n",
    "    payload = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = session.sagemaker_runtime_client\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name, ContentType=\"text/libsvm\", Body=payload\n",
    ")\n",
    "result = response[\"Body\"].read().decode(\"ascii\")\n",
    "print(\"Predicted values are {}.\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "When you're done with this exercise, please run the cell below to delete the hosted endpoint and avoid any additional charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
